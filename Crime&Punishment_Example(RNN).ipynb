{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пример генерации текста\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.recurrent import SimpleRNN, LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from input...\n"
     ]
    }
   ],
   "source": [
    "INPUT_FILE = \"./data/11-0.txt\"\n",
    "INPUT_FILE = \"./data/Dostoevskiy.txt\"\n",
    "# extract the input as a stream of characters\n",
    "print(\"Extracting text from input...\")\n",
    "fin = open(INPUT_FILE, 'r', encoding='utf-8')\n",
    "lines = []\n",
    "for line in fin:\n",
    "    line = line.strip().lower()\n",
    "    if len(line) == 0:\n",
    "        continue\n",
    "    lines.append(line)\n",
    "fin.close()\n",
    "text = \" \".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating lookup tables\n",
    "# Here chars is the number of features in our character \"vocabulary\"\n",
    "chars = set([c for c in text])\n",
    "nb_chars = len(chars)\n",
    "char2index = dict((c, i) for i, c in enumerate(chars))\n",
    "index2char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating input and label text...\n"
     ]
    }
   ],
   "source": [
    "# create inputs and labels from the text. We do this by stepping\n",
    "# through the text ${step} character at a time, and extracting a \n",
    "# sequence of size ${seqlen} and the next output char. For example,\n",
    "# assuming an input text \"The sky was falling\", we would get the \n",
    "# following sequence of input_chars and label_chars (first 5 only)\n",
    "#   The sky wa -> s\n",
    "#   he sky was ->  \n",
    "#   e sky was  -> f\n",
    "#    sky was f -> a\n",
    "#   sky was fa -> l\n",
    "print(\"Creating input and label text...\")\n",
    "SEQLEN = 10\n",
    "STEP = 1\n",
    "\n",
    "input_chars = []\n",
    "label_chars = []\n",
    "for i in range(0, len(text) - SEQLEN, STEP):\n",
    "    input_chars.append(text[i:i + SEQLEN])\n",
    "    label_chars.append(text[i + SEQLEN])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing input and label text...\n"
     ]
    }
   ],
   "source": [
    "# vectorize the input and label chars\n",
    "# Each row of the input is represented by seqlen characters, each \n",
    "# represented as a 1-hot encoding of size len(char). There are \n",
    "# len(input_chars) such rows, so shape(X) is (len(input_chars),\n",
    "# seqlen, nb_chars).\n",
    "# Each row of output is a single character, also represented as a\n",
    "# dense encoding of size len(char). Hence shape(y) is (len(input_chars),\n",
    "# nb_chars).\n",
    "print(\"Vectorizing input and label text...\")\n",
    "X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)\n",
    "y = np.zeros((len(input_chars), nb_chars), dtype=np.bool)\n",
    "for i, input_char in enumerate(input_chars):\n",
    "    for j, ch in enumerate(input_char):\n",
    "        X[i, j, char2index[ch]] = 1\n",
    "    y[i, char2index[label_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model. We use a single RNN with a fully connected layer\n",
    "# to compute the most likely predicted output char\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 128\n",
    "NUM_ITERATIONS = 25\n",
    "NUM_EPOCHS_PER_ITERATION = 1\n",
    "NUM_PREDS_PER_EPOCH = 100\n",
    "\n",
    "model = Sequential()\n",
    "#model.add(SimpleRNN(HIDDEN_SIZE, return_sequences=False,\n",
    "#                    input_shape=(SEQLEN, nb_chars),\n",
    "#                    unroll=True))\n",
    "model.add(LSTM(HIDDEN_SIZE, input_shape=(SEQLEN, nb_chars), dropout=0.2, recurrent_dropout=0.2, return_sequences=False, unroll=True))\n",
    "\n",
    "model.add(Dense(nb_chars))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Iteration #: 0\n",
      "Epoch 1/1\n",
      "1239568/1239568 [==============================] - 1062s - loss: 2.3858  \n",
      "Generating from seed:  еще довол\n",
      " еще довольников подом не посторил с не подом не посторил с не подом не посторил с не подом не посторил с не п\n",
      "==================================================\n",
      "Iteration #: 1\n",
      "Epoch 1/1\n",
      "1239568/1239568 [==============================] - 1048s - loss: 2.0833  \n",
      "Generating from seed: ром тела, \n",
      "ром тела, не стало в помнил он принять в том в помнил он принять в том в помнил он принять в том в помнил он п\n",
      "==================================================\n",
      "Iteration #: 2\n",
      "Epoch 1/1\n",
      "1239568/1239568 [==============================] - 1070s - loss: 1.9968  \n",
      "Generating from seed: его взгляд\n",
      "его взглядел он всё это все дела под бестровить на него просто и просто и просто и просто и просто и просто и \n",
      "==================================================\n",
      "Iteration #: 3\n",
      "Epoch 1/1\n",
      "1239568/1239568 [==============================] - 1048s - loss: 1.9548  \n",
      "Generating from seed:  не обнару\n",
      " не обнарую и при себя на него всё в комнату в комнату в комнату в комнату в комнату в комнату в комнату в ком\n",
      "==================================================\n",
      "Iteration #: 4\n",
      "Epoch 1/1\n",
      "1239568/1239568 [==============================] - 1039s - loss: 1.9318  \n",
      "Generating from seed: . однажды,\n",
      ". однажды, не под будет на него всегда на него всегда на него всегда на него всегда на него всегда на него все\n",
      "==================================================\n",
      "Iteration #: 5\n",
      "Epoch 1/1\n",
      "1239568/1239568 [==============================] - 1038s - loss: 1.9169  \n",
      "Generating from seed: вскричал о\n",
      "вскричал он в собственно подобривал он в собственно подобривал он в собственно подобривал он в собственно подо\n",
      "==================================================\n",
      "Iteration #: 6\n",
      "Epoch 1/1\n",
      "1239568/1239568 [==============================] - 1035s - loss: 1.9053  \n",
      "Generating from seed: ринимал тя\n",
      "ринимал тяже собственно под возможно под возможно под возможно под возможно под возможно под возможно под возм\n",
      "==================================================\n",
      "Iteration #: 7\n",
      "Epoch 1/1\n",
      "1239568/1239568 [==============================] - 1040s - loss: 1.8977  \n",
      "Generating from seed:  мягкие по\n",
      " мягкие подумал он подумал он подумал он подумал он подумал он подумал он подумал он подумал он подумал он под\n",
      "==================================================\n",
      "Iteration #: 8\n",
      "Epoch 1/1\n",
      "1239568/1239568 [==============================] - 1043s - loss: 1.8911  \n",
      "Generating from seed: ников. — т\n",
      "ников. — так в собственно стал он в своей стороны и соня соня с него в своей стороны и соня соня с него в свое\n",
      "==================================================\n",
      "Iteration #: 9\n",
      "Epoch 1/1\n",
      "1239568/1239568 [==============================] - 1046s - loss: 1.8865  \n",
      "Generating from seed:  да и не о\n",
      " да и не ответил он всё это всё это всё это всё это всё это всё это всё это всё это всё это всё это всё это вс\n",
      "==================================================\n",
      "Iteration #: 10\n",
      "Epoch 1/1\n",
      "1239568/1239568 [==============================] - 1050s - loss: 1.8839  \n",
      "Generating from seed: забита и н\n",
      "забита и не под всем под всем под всем под всем под всем под всем под всем под всем под всем под всем под всем\n",
      "==================================================\n",
      "Iteration #: 11\n",
      "Epoch 1/1\n",
      "1239568/1239568 [==============================] - 1067s - loss: 1.8812  \n",
      "Generating from seed: о разбират\n",
      "о разбирать на него всегда на него всегда на него всегда на него всегда на него всегда на него всегда на него \n",
      "==================================================\n",
      "Iteration #: 12\n",
      "Epoch 1/1\n",
      "1239568/1239568 [==============================] - 1048s - loss: 1.8786  \n",
      "Generating from seed: ать-с? — и\n",
      "ать-с? — и вы всем подумал он в своей под возможно в своей под возможно в своей под возможно в своей под возмо\n",
      "==================================================\n",
      "Iteration #: 13\n",
      "Epoch 1/1\n",
      "1239568/1239568 [==============================] - 1060s - loss: 1.8758  \n",
      "Generating from seed: например… \n",
      "например… — подумал он под всем под всем под всем под всем под всем под всем под всем под всем под всем под вс\n",
      "==================================================\n",
      "Iteration #: 14\n",
      "Epoch 1/1\n",
      "1239568/1239568 [==============================] - 1059s - loss: 1.8749  \n",
      "Generating from seed: вум. вы ру\n",
      "вум. вы руки в своей под вам в своей под вам в своей под вам в своей под вам в своей под вам в своей под вам в\n",
      "==================================================\n",
      "Iteration #: 15\n",
      "Epoch 1/1\n",
      "1239568/1239568 [==============================] - 1066s - loss: 1.8725  \n",
      "Generating from seed: , и там вс\n",
      ", и там всегда на него всегда на него всегда на него всегда на него всегда на него всегда на него всегда на не\n",
      "==================================================\n",
      "Iteration #: 16\n",
      "Epoch 1/1\n",
      "1239568/1239568 [==============================] - 1090s - loss: 1.8720  \n",
      "Generating from seed: а стояла п\n",
      "а стояла подошел он всё это всё это всё это всё это всё это всё это всё это всё это всё это всё это всё это вс\n",
      "==================================================\n",
      "Iteration #: 17\n",
      "Epoch 1/1\n",
      "1239568/1239568 [==============================] - 965s - loss: 1.8711   \n",
      "Generating from seed:  ей самое \n",
      " ей самое странно с ней под воскольников под воскольников под воскольников под воскольников под воскольников п\n",
      "==================================================\n",
      "Iteration #: 18\n",
      "Epoch 1/1\n",
      "1239568/1239568 [==============================] - 950s - loss: 1.8705   \n",
      "Generating from seed: алось, всё\n",
      "алось, всё это вы всегда на него в своего подумал он вы всегда на него в своего подумал он вы всегда на него в\n",
      "==================================================\n",
      "Iteration #: 19\n",
      "Epoch 1/1\n",
      "1239568/1239568 [==============================] - 948s - loss: 1.8706   - ETA: 0s - loss: 1.8\n",
      "Generating from seed:  к заметов\n",
      " к заметов и при всем под воскольников и при всем под воскольников и при всем под воскольников и при всем под \n",
      "==================================================\n",
      "Iteration #: 20\n",
      "Epoch 1/1\n",
      "1239568/1239568 [==============================] - 950s - loss: 1.8705   \n",
      "Generating from seed: , что они \n",
      ", что они с него в себя с него в себя с него в себя с него в себя с него в себя с него в себя с него в себя с \n",
      "==================================================\n",
      "Iteration #: 21\n",
      "Epoch 1/1\n",
      "1239568/1239568 [==============================] - 947s - loss: 1.8713   \n",
      "Generating from seed:  уставу ра\n",
      " уставу раскольников под воскольников под воскольников под воскольников под воскольников под воскольников под \n",
      "==================================================\n",
      "Iteration #: 22\n",
      "Epoch 1/1\n",
      "1239568/1239568 [==============================] - 947s - loss: 1.8707   \n",
      "Generating from seed:  подхватил\n",
      " подхватил он под воскольников под воскольников под воскольников под воскольников под воскольников под восколь\n",
      "==================================================\n",
      "Iteration #: 23\n",
      "Epoch 1/1\n",
      "1239568/1239568 [==============================] - 943s - loss: 1.8708   \n",
      "Generating from seed:  он вдруг \n",
      " он вдруг подумал он подумал он подумал он подумал он подумал он подумал он подумал он подумал он подумал он п\n",
      "==================================================\n",
      "Iteration #: 24\n",
      "Epoch 1/1\n",
      "1239568/1239568 [==============================] - 1089s - loss: 1.8716  \n",
      "Generating from seed: ассе, а с \n",
      "ассе, а с него не под восколько стало по стало по стало по стало по стало по стало по стало по стало по стало \n"
     ]
    }
   ],
   "source": [
    "# We train the model in batches and test output generated at each step\n",
    "for iteration in range(NUM_ITERATIONS):\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Iteration #: %d\" % (iteration))\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION)\n",
    "    \n",
    "    # testing model\n",
    "    # randomly choose a row from input_chars, then use it to \n",
    "    # generate text from model for next 100 chars\n",
    "    test_idx = np.random.randint(len(input_chars))\n",
    "    test_chars = input_chars[test_idx]\n",
    "    print(\"Generating from seed: %s\" % (test_chars))\n",
    "    print(test_chars, end=\"\")\n",
    "    for i in range(NUM_PREDS_PER_EPOCH):\n",
    "        Xtest = np.zeros((1, SEQLEN, nb_chars))\n",
    "        for i, ch in enumerate(test_chars):\n",
    "            Xtest[0, i, char2index[ch]] = 1\n",
    "        pred = model.predict(Xtest, verbose=0)[0]\n",
    "        ypred = index2char[np.argmax(pred)]\n",
    "        print(ypred, end=\"\")\n",
    "        # move forward with test_chars + ypred\n",
    "        test_chars = test_chars[1:] + ypred\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
