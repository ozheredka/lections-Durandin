{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Лекция 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перцептрон (perceptron) - простой алгоритм, который по заданному вектору $x = (x_1, x_2, \\ldots, x_n)$, называемому входными атрибутами, дает либо 1 (да), либо 0.\n",
    "Математически, это выражается функцией:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x) = \\begin{cases} 1, & wx + b > 0 \\\\ 0, & \\mbox{otherwise }\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "где $w$ - вектор весовых коэффициентов, а $wx$ - скалярное произведение: $\\sum_{j=1}^{m}w_jx_j$, и $b$ - смещение (bias).\n",
    "\n",
    "Если вспоминать геометрию, то $wx+b$ определяет границы гиперплоскости.\n",
    "\n",
    "Если $x$ лежит над гиперплоскостью, то решение уравнения $wx+b>0$, а если под  гиперплоскостью, то $wx+b<0$.\n",
    "**Перцептрон не может отвечать на вопрос \"может быть\"** Он может ответить на простой вопрос **да (1)** или **нет (0)**, если у нас есть понимание как именно определить $w$ и $b$ (которые определяются в ходе процесса обучения)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Первый пример использования Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первичный базовый блок Keras - это **модель**, и простейшая модель называется **последовательной (sequential)**. Последовательная модель Keras - просто линейный конвейер (стек) слоев нейронной сети.\n",
    "\n",
    "Фрагмент кода ниже определяет один слой с 12 нейронами, и он ожидает 8 входных переменных (атрибутов):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim = 8, kernel_initializer = 'random_uniform'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый нейрон инициализируется конкретными весами. Keras предоставляет несколько возможнестей для начальной инициализации:\n",
    "* __random_uniform__: Веса инициализируются равномерно случайными значениями в интервале $(-0.05; 0.05)$.\n",
    "Другими словами, каждое значение в заданном интервале равновероятно.\n",
    "\n",
    "* __random_normal__: Веса инициализируются в соответствии с Гауссовским распределением с нулевым матожиданием и малым стандартным отклонением $\\sigma = 0.05$.\n",
    "\n",
    "* __zero__ : Веса инициализируются нулевыми значениями;\n",
    "\n",
    "\n",
    "Полный список: https://keras.io/initializers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Многослойный перцептрон\n",
    "\n",
    "Рассмотрим пример нейронной сети с несколькими линейными слоями. Исторически, перцептрон было именем данным модели с одним линейным слоем.\n",
    "Как следствие, если он имеет несколько слоев, то он будет называться **многослойным перцептроном (multilayer perceptron, MLP)**.\n",
    "\n",
    "На рис. изображена общая нейронная сеть с одним входным слоем (input layer), одним промежуточным слоем (intermediate layer) и одним выходным слоем (output layer).\n",
    "\n",
    "![](https://www.safaribooksonline.com/library/view/deep-learning-with/9781787128422/assets/B06258_01_02.png)\n",
    "\n",
    "В представлененой диаграмме, каждый узел первого слоя получает на вход данные, и реагирует в соответствии с предопределенными локальными решающими границами.\n",
    "Выход первого слоя передается на второй слой, результаты которого передаются на последний выходной уровень, содержащий всего один нейрон.\n",
    "\n",
    "Приведенная архитектура является примером плотной сети (dense network).\n",
    "**Сеть плотная (dense), что означает что каждый нейрон в слое соединен со всеми нейронами предыдущего слоя и всеми нейронами последующего слоя.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проблемы при обучении перцептрона\n",
    "\n",
    "Рассмотрим единичный нейрон, как наилучшим образом выбрать вес $w$ и смещение (bies) $b$. В идеале, мы бы хотели предоставить множество обучающих примеров и дать компьютеру возможность настроить веса и смещение так, чтобы минимизировать число возникающих ошибок.\n",
    "\n",
    "Для того, чтобы быть более конкретными, предположим, что у нас есть множество картинок кошек и другое множество картинок, на которых нет кошек. \n",
    "\n",
    "Для простоты, предположим что каждый нейрон анализирует один пиксель исходной картинки. Мы хотели бы, чтобы в процессе обработки изображений нейрон изменял свой вес и смещение так, чтобы число изображений, распознанных как не кошки со временем уменьшалось. Этот подход интуитивно очевиден, но для него требуется, чтобы малое изменение веса (и\\или смещения) приводило к малому изменению результата.\n",
    "\n",
    "Если имеется большой скачок га выходе, то прогрессивное обучение невозможно (разве что пробовать всевозможные направления). В конце концов, дети учаться постепено. Для перцептрона же такое \"постепенное\" поведение нехарактерно. Перцептрон выдает значение 0, или 1 - дискретное значение.\n",
    "\n",
    "Вместо этого необходима гладкая - непрерывная дифференцируемая функция, монотонно возрастающая на отрезке $[0;1]$.\n",
    "\n",
    "## Сигмоида\n",
    "\n",
    "Сигмоидальная функция определяется сл. образом: $$\\sigma(x) = \\frac{1}{1+e^{-x}}$$.\n",
    "\n",
    "Сигмоидальная функция непрерывна и изменяется от 0 до 1, когда аргумент пробегает область определения $(-\\inf; \\inf)$.\n",
    "\n",
    "![](https://www.safaribooksonline.com/library/view/deep-learning-with/9781787128422/assets/B06258_01_05.png)\n",
    "\n",
    "Нейрон может воспользоваться сигмоидальной функцией для вычисления нелинейной функции $\\sigma(z = wx+b)$. Отметим, что когда величина $z=wx+b$ очень велика и положительна, $e^{-z} \\to 0$, так что $\\sigma(z)\\to 1$, а когда эта величина велика по модулю и отрицательна, то $e^{-z} \\to \\inf$, так что $\\sigma(z)\\to 0$.\n",
    "Иными словами, нейрон с сигмоидной функцией активации ведет себя подобно перцептрону, но изменяется плавно и может порождать такие значения, как 0.5539 или 0.123191. В некотором смысле, нейрон с сигмоидной функцией активации умеет давать ответ *может быть*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Блок линейной ректификации (ReLU)\n",
    "\n",
    "В последнее время стала популярна совсем простая функция активации, называемая **блоком линейной ректификации (rectified linear unit ReLU)**, поскольку в экспериментах она дает замечательные результаты.\n",
    "Определяется ReLU формулой:\n",
    "$$ f(x) = \\max(0, x)$$\n",
    "График функции ReLU показан на рисунке:\n",
    "\n",
    "![](https://www.safaribooksonline.com/library/view/deep-learning-with/9781787128422/assets/B06258_01_06.png)\n",
    "\n",
    "## Функции активации\n",
    "\n",
    "Сигмоида и ReLU называются функциями активации. Непрерывное изменение, характерное для этих функций крайне важно для разработки алгоритмов обучения, которые адаптируются постепенно, стремясь уменьшить ошибку сети. \n",
    "\n",
    "На рисунке ниже проиллюстрирована схема применения функции активации $\\sigma$ к входному вектору $(x_1, x_2, \\ldots, x_m)$ вектору весов $(w_1, w_2, \\ldots, w_m)$, смещению $b$ и сумматору $\\Sigma$:\n",
    "\n",
    "![](https://www.safaribooksonline.com/library/view/deep-learning-with/9781787128422/assets/B06258_01_07.png)\n",
    "\n",
    "Keras подерживает несколько функций активации, полный перечень приведен в документации: https://keras.io/activations/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Реальный пример - распознавание рукописных цифр\n",
    "\n",
    "Построим сеть, умеющую распознавать рукописные цифры.\n",
    "\n",
    "* Набор данных MNIST: 60 000 обучающих и 10 000 тестовых примеров;\n",
    "* Все изображения полутоновые, размера 28x28 пикселей.\n",
    "\n",
    "![](https://www.safaribooksonline.com/library/view/deep-learning-with/9781787128422/assets/B06258_01_08.png)\n",
    "\n",
    "## Унитарное (one hot) кодирование\n",
    "\n",
    "Во многих приложениях удобно преобразовывать категориальные признаки в числовые. апример категориальный признак - цифру, принимающую значение $d$ от 0 до 9, - можно представить бинарным вектором длины 10, в котором $d$-й элемент равен 1, а остальные 0.\n",
    "\n",
    "Такое представление называется **унитарным кодированием (one-hot encoding)** и часто применяется в анализе данных.\n",
    "\n",
    "## Определение простой нейронной сети в Keras\n",
    "\n",
    "Итак, будем использовать библиотеку Keras для определения нейронной сети, распознающей рукописные цифры из набора MNIST.\n",
    "\n",
    "Keras предоставляет средства для закрузки набора данных и разбиения его на обучающую и тестовые выборки. Для поддержки вычислений на GPU данные преобразуются к типу float32 и нормируются на интервал [0;1].\n",
    "Также проводится унитарное кодирование."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "\n",
    "np.random.seed(1671) # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# network and training\n",
    "NB_EPOCH = 200\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # SGD optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11419648/11490434 [============================>.] - ETA: 0s60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# data: shuffled and split between train and test sets\n",
    "#\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# normalize\n",
    "#\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во входном слое с каждым пикселем изображения будет ассоциирован один нейрон, т.е. всего получается 28x28 = 784 нейрона.\n",
    "\n",
    "Обычно значения ассоциированные с пикселями нормируются для приведения к диапазону [0;1]. На выходе получается 10 классов, по одному для каждой цифры.\n",
    "\n",
    "Последний слой состоит из единственного нейрона с функцией активации **softmax**, являющейся обобщением сигмоидальной функции. Softmax \"сплющивает\" k-мерный вектор, содржащий произвольные вещественные числа в $k$-мерный вектор вещественных чисел из интервала (0;1). \n",
    "В нашем случае, она агрегирует 10 ответов, выданных предыдущем слоем из 10 нейронов.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(NB_CLASSES, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определенную таким образом модель необходимо откомпилировать, т.е. привести к виду, допускабщему использование бащовой библиотекой (Theano\\TensorFlow). Перед компиляцией необходимо определить с выбором:\n",
    "* оптимизатора, т.е. выбрать конкретный алгоритм, который будет обновлять веса в процессе обучения модели;\n",
    "* целевой функции (функции потерь), которую оптимизатор использует для навигации по пространству весов;\n",
    "* оценки качества обученной модели.\n",
    "\n",
    "Несколько широко распространенных целевых функций (функций потерь).\n",
    "Полный список приведен: https://keras.io/losses/\n",
    "\n",
    "* Среднеквадратичная ошибка (MSE)\n",
    "$$MSE = \\frac{1}{n}\\sum_{i=1}^{n}(\\hat{Y} - Y)^2$$\n",
    "где $\\hat{Y}$ - вектор $n$ предсказаний, а $Y$ - вектор $n$ наблюдаемых значений.\n",
    "*Если предсказание сильно отличается от истинного значения, то возвдение квадрат делает отличие еще более выраженным*\n",
    "\n",
    "\n",
    "* Бинарная перекрестная энтропия (binary cross-entropy\\)\n",
    "Если модель предсказывает значение $p$, тогда как истинное значение равно $t$, то бинарная перекрестная энтропия равна:\n",
    "$$ -t\\log(p) - (1-t)\\log(1-p)$$\n",
    "*Эта целевая функция подходит для предсказания бинарных меток*\n",
    "\n",
    "\n",
    "* Категориальная перекрестная энтропия\n",
    "Представляет собой логарифмическую потерю в случае нескольких классов. Если модель предсказывает значения $p_{i,j}$, тогда как истинные значения равны $t_{i,j}$, то категориальная перекрестная энтропия равна:\n",
    "$$L_{i} = -\\sum_{j}t_{i,j}\\log(p_{i,j})$$\n",
    "*Эта целевая функция подходит для многоклассовой классификации. По умолчанию она использщуется совместно с функцией активации softmax*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наиболее популярные показатели качества:\n",
    "* Верность (accuracy) - отношение числа правильных предсказаний к общему числу меток;\n",
    "* Точность (precision) - доля правильных ответов модели;\n",
    "* Полнота (recall) - доля обнаруженных истинных событий.\n",
    "\n",
    "Полный список: https://keras.io/metrics/\n",
    "\n",
    "Показатели качества напоминают целевые функции. Различаются же они тем, что показатели используются не для обучения модели, а для оценки её качества. Компиляция модели в Keras производится сл. образом:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обучения откомпилированной модели служит функция *fit()*, принимающая следующие параметры:\n",
    "* **epochs**: число периодов (эпох) - сколько раз обучающий набор \"предъявляется\" модели. На каждой итерации оптимизатор пытается подкорректировать веса, стремясь минимизировать целевую функцию;\n",
    "* **batch_size** : сколько обучающих примеров должен увидеть оптимизатор прежде чем он обновит веса.\n",
    "\n",
    "В Keras обучение модели достаточно простое. Аналогично обучению во фреймворке scikit-learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 1s - loss: 0.3367 - acc: 0.9065 - val_loss: 0.3198 - val_acc: 0.9112\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 1s - loss: 0.3353 - acc: 0.9071 - val_loss: 0.3187 - val_acc: 0.9120\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - ETA: 0s - loss: 0.3341 - acc: 0.906 - 1s - loss: 0.3340 - acc: 0.9070 - val_loss: 0.3176 - val_acc: 0.9119\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 1s - loss: 0.3327 - acc: 0.9079 - val_loss: 0.3167 - val_acc: 0.9116\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 1s - loss: 0.3315 - acc: 0.9081 - val_loss: 0.3157 - val_acc: 0.9119\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 1s - loss: 0.3303 - acc: 0.9083 - val_loss: 0.3147 - val_acc: 0.9128\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 1s - loss: 0.3291 - acc: 0.9085 - val_loss: 0.3138 - val_acc: 0.9126\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 1s - loss: 0.3281 - acc: 0.9087 - val_loss: 0.3129 - val_acc: 0.9134\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 1s - loss: 0.3269 - acc: 0.9093 - val_loss: 0.3123 - val_acc: 0.9137\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 1s - loss: 0.3260 - acc: 0.9091 - val_loss: 0.3113 - val_acc: 0.9133\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE, epochs=10,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*В этом примере часть обучающего набора зарезервирована для контроля.*\n",
    "\n",
    "\n",
    "После обучения модели, проверку следует проводить на тестовом наборе. Таким образом, мы сможем получить минимальное значение, достигаемое целевой функцией, и наилучшее значение показателя качества.\n",
    "\n",
    "\n",
    "**Тренировочная и тестовая выборка не должны пересекаться**.\n",
    "(Вспоминаем в чем смысл обучения?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9088/10000 [==========================>...] - ETA:  - ETA: 0s\n",
      "Test score: 0.310620229274\n",
      "Test accuracy: 0.9144\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print()\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, проверка модели:\n",
    "* тренировочная выборка: 0.9091\n",
    "* контрольная: 0.9133\n",
    "* тестовая выборка: 0.9144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Улучшение нейронной сети посредством добавления скрытых слоев\n",
    "\n",
    "Первым улучшением предложенной нейронной сети будет включение в сеть дополнительных слоёв. После входного слова поместим первый плотный слой с **N_HIDDEN** нейронами и функцией активации ReLU. Этот слой называется скрытым, потому что он напрямую не соединен ни с входом ни с выходом. После первого скрытого слоя добавим еще один, также содержащий **N_HIDDEN** нейронов, а уже за ним будет выходной слов с 10 нейронами.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# network and training\n",
    "NB_EPOCH = 10\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10   # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# normalize \n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 3s - loss: 1.4233 - acc: 0.6445 - val_loss: 0.7049 - val_acc: 0.8469\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 3s - loss: 0.5661 - acc: 0.8603 - val_loss: 0.4369 - val_acc: 0.8857\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 3s - loss: 0.4222 - acc: 0.8857 - val_loss: 0.3637 - val_acc: 0.9001\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 3s - loss: 0.3685 - acc: 0.8984 - val_loss: 0.3293 - val_acc: 0.9081\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 3s - loss: 0.3383 - acc: 0.9059 - val_loss: 0.3069 - val_acc: 0.9137\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 3s - loss: 0.3171 - acc: 0.9106 - val_loss: 0.2911 - val_acc: 0.9187\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 3s - loss: 0.3004 - acc: 0.9156 - val_loss: 0.2799 - val_acc: 0.9217\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 3s - loss: 0.2870 - acc: 0.9185 - val_loss: 0.2675 - val_acc: 0.9254\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 3s - loss: 0.2749 - acc: 0.9215 - val_loss: 0.2571 - val_acc: 0.9266\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 3s - loss: 0.2646 - acc: 0.9256 - val_loss: 0.2494 - val_acc: 0.9284\n",
      " 9600/10000 [===========================>..] - ETA: 0s\n",
      "Test score: 0.24968713724315167\n",
      "Test accuracy: 0.9299\n"
     ]
    }
   ],
   "source": [
    "#convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# M_HIDDEN hidden layers\n",
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=OPTIMIZER,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "                    verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"\\nTest score: {}\\nTest accuracy: {}\".format(score[0], score[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавив два скрытых слоя, мы достигли верности:\n",
    "* на обучающем наборе: 0.9256\n",
    "* на контрольном наборе: 0.9284\n",
    "* на тестовом наборе: 0.9299\n",
    "\n",
    "** Задание: как изменится точность если вместо двух скрытых слоев добавить один? **\n",
    "\n",
    "** Задание: как изменится точность, если добавить больше двух скрытых слоёв?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дальнейшее улучшение простой сети Keras: прореживание (Dropout)\n",
    "\n",
    "Применим еще одно достаточно простое улучшение. Добавим т.н. **прореживание** - с вероятностью *dropout* будем случайным образом отбрасывать некоторые значения, распространяющиеся внутри сети, состоящей из плотных скрытых лоев. Это хорошо известная форма регуляризации. \n",
    "\n",
    "Как ни странно, отбрасывание некоторых значений приводит к улучшению качества.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 4s - loss: 1.7832 - acc: 0.4300 - val_loss: 0.9561 - val_acc: 0.8139\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 3s - loss: 0.9516 - acc: 0.7119 - val_loss: 0.5420 - val_acc: 0.8693\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 3s - loss: 0.7095 - acc: 0.7826 - val_loss: 0.4303 - val_acc: 0.8872\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 3s - loss: 0.6028 - acc: 0.8160 - val_loss: 0.3774 - val_acc: 0.8965\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 3s - loss: 0.5377 - acc: 0.8405 - val_loss: 0.3428 - val_acc: 0.9043\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 3s - loss: 0.4948 - acc: 0.8517 - val_loss: 0.3183 - val_acc: 0.9106\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 3s - loss: 0.4639 - acc: 0.8624 - val_loss: 0.2984 - val_acc: 0.9156\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 3s - loss: 0.4327 - acc: 0.8729 - val_loss: 0.2829 - val_acc: 0.9189\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 3s - loss: 0.4123 - acc: 0.8773 - val_loss: 0.2701 - val_acc: 0.9225\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 3s - loss: 0.3959 - acc: 0.8823 - val_loss: 0.2585 - val_acc: 0.9262\n",
      " 9248/10000 [==========================>...] - ETA: 0s\n",
      "Test score: 0.25957589367330075\n",
      "Test accuracy: 0.9258\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "# network and training\n",
    "NB_EPOCH = 10\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10   # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "DROPOUT = 0.3\n",
    "\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# normalize \n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# M_HIDDEN hidden layers\n",
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=OPTIMIZER,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "                    verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"\\nTest score: {}\\nTest accuracy: {}\".format(score[0], score[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как повела себя нейронная сеть в результате добавления прореживания? Выполнив те же 10 итераций, что и раньше получим сл. изменения верности:\n",
    "* на обучающем наборе: 0.9256 => **0.8823**\n",
    "* на контрольном наборе: 0.9284 => **0.9262**\n",
    "* на тестовом наборе: 0.9299 => **0.9258**\n",
    "\n",
    "Что-то как то ни о чем!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Следует заметить, что верность на обучающем наборе должна быть выше, чем на тестовом, в противном случае мы прервали обучение слишком рано.**\n",
    "\n",
    "Увеличим число периодов до 30!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.2003 - acc: 0.9414 - val_loss: 0.1377 - val_acc: 0.9597\n",
      "Epoch 2/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.1965 - acc: 0.9424 - val_loss: 0.1360 - val_acc: 0.9605\n",
      "Epoch 3/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.1935 - acc: 0.9431 - val_loss: 0.1344 - val_acc: 0.9612\n",
      "Epoch 4/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.1904 - acc: 0.9442 - val_loss: 0.1335 - val_acc: 0.9617\n",
      "Epoch 5/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1894 - acc: 0.9438 - val_loss: 0.1321 - val_acc: 0.9616\n",
      "Epoch 6/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.1858 - acc: 0.9455 - val_loss: 0.1311 - val_acc: 0.9615\n",
      "Epoch 7/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.1848 - acc: 0.9451 - val_loss: 0.1299 - val_acc: 0.9614\n",
      "Epoch 8/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.1827 - acc: 0.9464 - val_loss: 0.1285 - val_acc: 0.9617\n",
      "Epoch 9/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.1802 - acc: 0.9466 - val_loss: 0.1272 - val_acc: 0.9628\n",
      "Epoch 10/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.1775 - acc: 0.9468 - val_loss: 0.1263 - val_acc: 0.9627\n",
      "Epoch 11/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.1777 - acc: 0.9480 - val_loss: 0.1247 - val_acc: 0.9635\n",
      "Epoch 12/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.1776 - acc: 0.9471 - val_loss: 0.1237 - val_acc: 0.9642\n",
      "Epoch 13/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.1707 - acc: 0.9492 - val_loss: 0.1223 - val_acc: 0.9644\n",
      "Epoch 14/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.1697 - acc: 0.9494 - val_loss: 0.1225 - val_acc: 0.9642\n",
      "Epoch 15/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.1688 - acc: 0.9495 - val_loss: 0.1205 - val_acc: 0.9641\n",
      "Epoch 16/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1663 - acc: 0.9510 - val_loss: 0.1197 - val_acc: 0.9647\n",
      "Epoch 17/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1642 - acc: 0.9515 - val_loss: 0.1191 - val_acc: 0.9653\n",
      "Epoch 18/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1648 - acc: 0.9515 - val_loss: 0.1180 - val_acc: 0.9653\n",
      "Epoch 19/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1631 - acc: 0.9518 - val_loss: 0.1168 - val_acc: 0.9658\n",
      "Epoch 20/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1565 - acc: 0.9543 - val_loss: 0.1172 - val_acc: 0.9651\n",
      "Epoch 21/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1600 - acc: 0.9525 - val_loss: 0.1155 - val_acc: 0.9661\n",
      "Epoch 22/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1574 - acc: 0.9530 - val_loss: 0.1143 - val_acc: 0.9662\n",
      "Epoch 23/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1527 - acc: 0.9544 - val_loss: 0.1140 - val_acc: 0.9659\n",
      "Epoch 24/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1540 - acc: 0.9535 - val_loss: 0.1137 - val_acc: 0.9662\n",
      "Epoch 25/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.1524 - acc: 0.9541 - val_loss: 0.1123 - val_acc: 0.9659\n",
      "Epoch 26/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1529 - acc: 0.9549 - val_loss: 0.1114 - val_acc: 0.9668\n",
      "Epoch 27/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1493 - acc: 0.9557 - val_loss: 0.1106 - val_acc: 0.9667\n",
      "Epoch 28/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1490 - acc: 0.9557 - val_loss: 0.1103 - val_acc: 0.9669\n",
      "Epoch 29/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1459 - acc: 0.9565 - val_loss: 0.1095 - val_acc: 0.9672\n",
      "Epoch 30/200\n",
      "48000/48000 [==============================] - 5s - loss: 0.1478 - acc: 0.9563 - val_loss: 0.1087 - val_acc: 0.9672\n",
      "Epoch 31/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.1432 - acc: 0.9580 - val_loss: 0.1085 - val_acc: 0.9679\n",
      "Epoch 32/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.1424 - acc: 0.9579 - val_loss: 0.1079 - val_acc: 0.9680\n",
      "Epoch 33/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.1428 - acc: 0.9577 - val_loss: 0.1077 - val_acc: 0.9681\n",
      "Epoch 34/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1419 - acc: 0.9580 - val_loss: 0.1068 - val_acc: 0.9680\n",
      "Epoch 35/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1402 - acc: 0.9580 - val_loss: 0.1058 - val_acc: 0.9683\n",
      "Epoch 36/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1370 - acc: 0.9588 - val_loss: 0.1061 - val_acc: 0.9681\n",
      "Epoch 37/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1374 - acc: 0.9582 - val_loss: 0.1052 - val_acc: 0.9688\n",
      "Epoch 38/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1361 - acc: 0.9600 - val_loss: 0.1045 - val_acc: 0.9697\n",
      "Epoch 39/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1367 - acc: 0.9589 - val_loss: 0.1050 - val_acc: 0.9692\n",
      "Epoch 40/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1368 - acc: 0.9604 - val_loss: 0.1043 - val_acc: 0.96910.9\n",
      "Epoch 41/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1327 - acc: 0.9607 - val_loss: 0.1039 - val_acc: 0.9688\n",
      "Epoch 42/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1316 - acc: 0.9604 - val_loss: 0.1033 - val_acc: 0.9695\n",
      "Epoch 43/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1328 - acc: 0.9607 - val_loss: 0.1027 - val_acc: 0.9688\n",
      "Epoch 44/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1312 - acc: 0.9598 - val_loss: 0.1020 - val_acc: 0.9694\n",
      "Epoch 45/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1275 - acc: 0.9608 - val_loss: 0.1015 - val_acc: 0.9703\n",
      "Epoch 46/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1279 - acc: 0.9627 - val_loss: 0.1010 - val_acc: 0.9703\n",
      "Epoch 47/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.1292 - acc: 0.9612 - val_loss: 0.1009 - val_acc: 0.9701\n",
      "Epoch 48/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.1292 - acc: 0.9609 - val_loss: 0.0999 - val_acc: 0.9711\n",
      "Epoch 49/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1279 - acc: 0.9612 - val_loss: 0.0997 - val_acc: 0.9707\n",
      "Epoch 50/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1246 - acc: 0.9618 - val_loss: 0.0992 - val_acc: 0.9709\n",
      "Epoch 51/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1278 - acc: 0.9614 - val_loss: 0.0992 - val_acc: 0.9710\n",
      "Epoch 52/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.1261 - acc: 0.9628 - val_loss: 0.0985 - val_acc: 0.9705\n",
      "Epoch 53/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1247 - acc: 0.9632 - val_loss: 0.0983 - val_acc: 0.9710\n",
      "Epoch 54/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1214 - acc: 0.9638 - val_loss: 0.0975 - val_acc: 0.9712\n",
      "Epoch 55/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.1207 - acc: 0.9634 - val_loss: 0.0974 - val_acc: 0.9713\n",
      "Epoch 56/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1228 - acc: 0.9633 - val_loss: 0.0970 - val_acc: 0.9712\n",
      "Epoch 57/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1195 - acc: 0.9649 - val_loss: 0.0967 - val_acc: 0.9711\n",
      "Epoch 58/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1187 - acc: 0.9646 - val_loss: 0.0963 - val_acc: 0.9715\n",
      "Epoch 59/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1188 - acc: 0.9642 - val_loss: 0.0962 - val_acc: 0.9713\n",
      "Epoch 60/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.1168 - acc: 0.9646 - val_loss: 0.0955 - val_acc: 0.9725\n",
      "Epoch 61/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.1183 - acc: 0.9642 - val_loss: 0.0953 - val_acc: 0.9718\n",
      "Epoch 62/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1164 - acc: 0.9656 - val_loss: 0.0952 - val_acc: 0.9713\n",
      "Epoch 63/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.1149 - acc: 0.9660 - val_loss: 0.0949 - val_acc: 0.9717\n",
      "Epoch 64/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.1158 - acc: 0.9649 - val_loss: 0.0950 - val_acc: 0.9718\n",
      "Epoch 65/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.1143 - acc: 0.9655 - val_loss: 0.0945 - val_acc: 0.9720\n",
      "Epoch 66/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.1144 - acc: 0.9654 - val_loss: 0.0939 - val_acc: 0.9723\n",
      "Epoch 67/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.1117 - acc: 0.9665 - val_loss: 0.0931 - val_acc: 0.9723\n",
      "Epoch 68/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.1117 - acc: 0.9664 - val_loss: 0.0934 - val_acc: 0.9718\n",
      "Epoch 69/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1114 - acc: 0.9659 - val_loss: 0.0931 - val_acc: 0.9727\n",
      "Epoch 70/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.1116 - acc: 0.9663 - val_loss: 0.0930 - val_acc: 0.9721\n",
      "Epoch 71/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.1095 - acc: 0.9674 - val_loss: 0.0925 - val_acc: 0.9724\n",
      "Epoch 72/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.1084 - acc: 0.9668 - val_loss: 0.0923 - val_acc: 0.9726\n",
      "Epoch 73/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.1083 - acc: 0.9676 - val_loss: 0.0915 - val_acc: 0.9730\n",
      "Epoch 74/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.1099 - acc: 0.9665 - val_loss: 0.0912 - val_acc: 0.9730\n",
      "Epoch 75/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.1065 - acc: 0.9678 - val_loss: 0.0911 - val_acc: 0.9730\n",
      "Epoch 76/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.1052 - acc: 0.9673 - val_loss: 0.0913 - val_acc: 0.9730\n",
      "Epoch 77/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.1080 - acc: 0.9673 - val_loss: 0.0911 - val_acc: 0.9728\n",
      "Epoch 78/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1089 - acc: 0.9669 - val_loss: 0.0903 - val_acc: 0.9728\n",
      "Epoch 79/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1045 - acc: 0.9680 - val_loss: 0.0902 - val_acc: 0.9736\n",
      "Epoch 80/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1044 - acc: 0.9687 - val_loss: 0.0904 - val_acc: 0.9735\n",
      "Epoch 81/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.1032 - acc: 0.9688 - val_loss: 0.0903 - val_acc: 0.9731\n",
      "Epoch 82/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1042 - acc: 0.9679 - val_loss: 0.0901 - val_acc: 0.9733\n",
      "Epoch 83/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1058 - acc: 0.9673 - val_loss: 0.0902 - val_acc: 0.9736\n",
      "Epoch 84/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1034 - acc: 0.9686 - val_loss: 0.0897 - val_acc: 0.9733\n",
      "Epoch 85/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1029 - acc: 0.9686 - val_loss: 0.0891 - val_acc: 0.9738\n",
      "Epoch 86/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1025 - acc: 0.9683 - val_loss: 0.0885 - val_acc: 0.9742\n",
      "Epoch 87/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1005 - acc: 0.9689 - val_loss: 0.0890 - val_acc: 0.9739\n",
      "Epoch 88/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1019 - acc: 0.9695 - val_loss: 0.0887 - val_acc: 0.9738\n",
      "Epoch 89/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1009 - acc: 0.9693 - val_loss: 0.0879 - val_acc: 0.9747\n",
      "Epoch 90/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.1003 - acc: 0.9704 - val_loss: 0.0885 - val_acc: 0.9740\n",
      "Epoch 91/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0981 - acc: 0.9700 - val_loss: 0.0876 - val_acc: 0.9744\n",
      "Epoch 92/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0987 - acc: 0.9699 - val_loss: 0.0874 - val_acc: 0.9746\n",
      "Epoch 93/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0998 - acc: 0.9703 - val_loss: 0.0873 - val_acc: 0.9743\n",
      "Epoch 94/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0959 - acc: 0.9703 - val_loss: 0.0877 - val_acc: 0.9745\n",
      "Epoch 95/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0980 - acc: 0.9700 - val_loss: 0.0880 - val_acc: 0.9743\n",
      "Epoch 96/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0971 - acc: 0.9711 - val_loss: 0.0878 - val_acc: 0.9750\n",
      "Epoch 97/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0965 - acc: 0.9715 - val_loss: 0.0867 - val_acc: 0.9746\n",
      "Epoch 98/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0957 - acc: 0.9699 - val_loss: 0.0871 - val_acc: 0.9746\n",
      "Epoch 99/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0956 - acc: 0.9704 - val_loss: 0.0872 - val_acc: 0.9750\n",
      "Epoch 100/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0921 - acc: 0.9720 - val_loss: 0.0870 - val_acc: 0.9748\n",
      "Epoch 101/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0922 - acc: 0.9719 - val_loss: 0.0862 - val_acc: 0.9747\n",
      "Epoch 102/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0933 - acc: 0.9719 - val_loss: 0.0861 - val_acc: 0.9749\n",
      "Epoch 103/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0935 - acc: 0.9713 - val_loss: 0.0861 - val_acc: 0.9750\n",
      "Epoch 104/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0914 - acc: 0.9722 - val_loss: 0.0860 - val_acc: 0.9748\n",
      "Epoch 105/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0915 - acc: 0.9721 - val_loss: 0.0865 - val_acc: 0.9740\n",
      "Epoch 106/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0921 - acc: 0.9722 - val_loss: 0.0854 - val_acc: 0.9751\n",
      "Epoch 107/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0904 - acc: 0.9726 - val_loss: 0.0857 - val_acc: 0.9751\n",
      "Epoch 108/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0906 - acc: 0.9725 - val_loss: 0.0863 - val_acc: 0.9748\n",
      "Epoch 109/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0896 - acc: 0.9729 - val_loss: 0.0850 - val_acc: 0.9751\n",
      "Epoch 110/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0890 - acc: 0.9724 - val_loss: 0.0859 - val_acc: 0.9747\n",
      "Epoch 111/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0900 - acc: 0.9727 - val_loss: 0.0852 - val_acc: 0.9751\n",
      "Epoch 112/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0883 - acc: 0.9727 - val_loss: 0.0847 - val_acc: 0.9750\n",
      "Epoch 113/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0880 - acc: 0.9734 - val_loss: 0.0848 - val_acc: 0.9752\n",
      "Epoch 114/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0866 - acc: 0.9739 - val_loss: 0.0846 - val_acc: 0.9752\n",
      "Epoch 115/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0867 - acc: 0.9739 - val_loss: 0.0852 - val_acc: 0.9752\n",
      "Epoch 116/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0883 - acc: 0.9731 - val_loss: 0.0849 - val_acc: 0.9752\n",
      "Epoch 117/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0870 - acc: 0.9734 - val_loss: 0.0844 - val_acc: 0.9757\n",
      "Epoch 118/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0883 - acc: 0.9727 - val_loss: 0.0844 - val_acc: 0.9751\n",
      "Epoch 119/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0855 - acc: 0.9736 - val_loss: 0.0842 - val_acc: 0.9752\n",
      "Epoch 120/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0864 - acc: 0.9744 - val_loss: 0.0836 - val_acc: 0.9755\n",
      "Epoch 121/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0866 - acc: 0.9747 - val_loss: 0.0835 - val_acc: 0.9756\n",
      "Epoch 122/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0862 - acc: 0.9729 - val_loss: 0.0834 - val_acc: 0.9757\n",
      "Epoch 123/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0857 - acc: 0.9743 - val_loss: 0.0837 - val_acc: 0.9759\n",
      "Epoch 124/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0854 - acc: 0.9739 - val_loss: 0.0834 - val_acc: 0.9759\n",
      "Epoch 125/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0857 - acc: 0.9739 - val_loss: 0.0828 - val_acc: 0.9753\n",
      "Epoch 126/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0849 - acc: 0.9747 - val_loss: 0.0830 - val_acc: 0.9758\n",
      "Epoch 127/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 3s - loss: 0.0842 - acc: 0.9742 - val_loss: 0.0838 - val_acc: 0.9761\n",
      "Epoch 128/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0830 - acc: 0.9744 - val_loss: 0.0828 - val_acc: 0.9758\n",
      "Epoch 129/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0825 - acc: 0.9737 - val_loss: 0.0822 - val_acc: 0.9763\n",
      "Epoch 130/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0818 - acc: 0.9741 - val_loss: 0.0828 - val_acc: 0.9758\n",
      "Epoch 131/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0795 - acc: 0.9756 - val_loss: 0.0827 - val_acc: 0.9763\n",
      "Epoch 132/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0817 - acc: 0.9754 - val_loss: 0.0827 - val_acc: 0.9756\n",
      "Epoch 133/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0818 - acc: 0.9750 - val_loss: 0.0819 - val_acc: 0.9759\n",
      "Epoch 134/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0815 - acc: 0.9750 - val_loss: 0.0820 - val_acc: 0.9763\n",
      "Epoch 135/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0804 - acc: 0.9750 - val_loss: 0.0821 - val_acc: 0.9758\n",
      "Epoch 136/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0802 - acc: 0.9758 - val_loss: 0.0814 - val_acc: 0.9763\n",
      "Epoch 137/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0811 - acc: 0.9747 - val_loss: 0.0823 - val_acc: 0.9757\n",
      "Epoch 138/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0789 - acc: 0.9752 - val_loss: 0.0818 - val_acc: 0.9763\n",
      "Epoch 139/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0786 - acc: 0.9755 - val_loss: 0.0819 - val_acc: 0.9762\n",
      "Epoch 140/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0782 - acc: 0.9760 - val_loss: 0.0819 - val_acc: 0.9763\n",
      "Epoch 141/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0804 - acc: 0.9752 - val_loss: 0.0817 - val_acc: 0.9762\n",
      "Epoch 142/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0785 - acc: 0.9757 - val_loss: 0.0821 - val_acc: 0.9763\n",
      "Epoch 143/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0768 - acc: 0.9770 - val_loss: 0.0817 - val_acc: 0.9762\n",
      "Epoch 144/200\n",
      "48000/48000 [==============================] - 5s - loss: 0.0793 - acc: 0.9755 - val_loss: 0.0810 - val_acc: 0.9763\n",
      "Epoch 145/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0768 - acc: 0.9771 - val_loss: 0.0820 - val_acc: 0.9762\n",
      "Epoch 146/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0768 - acc: 0.9760 - val_loss: 0.0818 - val_acc: 0.9760\n",
      "Epoch 147/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0770 - acc: 0.9759 - val_loss: 0.0818 - val_acc: 0.9758\n",
      "Epoch 148/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0758 - acc: 0.9762 - val_loss: 0.0818 - val_acc: 0.9760\n",
      "Epoch 149/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0748 - acc: 0.9767 - val_loss: 0.0818 - val_acc: 0.9761\n",
      "Epoch 150/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0765 - acc: 0.9764 - val_loss: 0.0808 - val_acc: 0.9761\n",
      "Epoch 151/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0739 - acc: 0.9767 - val_loss: 0.0811 - val_acc: 0.9764\n",
      "Epoch 152/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0782 - acc: 0.9756 - val_loss: 0.0805 - val_acc: 0.9763\n",
      "Epoch 153/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0756 - acc: 0.9758 - val_loss: 0.0814 - val_acc: 0.9763\n",
      "Epoch 154/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0742 - acc: 0.9771 - val_loss: 0.0801 - val_acc: 0.9768\n",
      "Epoch 155/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0738 - acc: 0.9765 - val_loss: 0.0809 - val_acc: 0.9763\n",
      "Epoch 156/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0718 - acc: 0.9781 - val_loss: 0.0803 - val_acc: 0.9767\n",
      "Epoch 157/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0757 - acc: 0.9767 - val_loss: 0.0808 - val_acc: 0.9760\n",
      "Epoch 158/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0737 - acc: 0.9775 - val_loss: 0.0806 - val_acc: 0.9764\n",
      "Epoch 159/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0750 - acc: 0.9760 - val_loss: 0.0807 - val_acc: 0.9763\n",
      "Epoch 160/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0744 - acc: 0.9767 - val_loss: 0.0802 - val_acc: 0.9766\n",
      "Epoch 161/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0716 - acc: 0.9779 - val_loss: 0.0811 - val_acc: 0.9766\n",
      "Epoch 162/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0736 - acc: 0.9773 - val_loss: 0.0807 - val_acc: 0.9762\n",
      "Epoch 163/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0701 - acc: 0.9784 - val_loss: 0.0802 - val_acc: 0.9768\n",
      "Epoch 164/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0722 - acc: 0.9770 - val_loss: 0.0800 - val_acc: 0.9764\n",
      "Epoch 165/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0708 - acc: 0.9787 - val_loss: 0.0800 - val_acc: 0.9766\n",
      "Epoch 166/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0714 - acc: 0.9779 - val_loss: 0.0804 - val_acc: 0.9765\n",
      "Epoch 167/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0701 - acc: 0.9778 - val_loss: 0.0800 - val_acc: 0.9768\n",
      "Epoch 168/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0705 - acc: 0.9778 - val_loss: 0.0804 - val_acc: 0.9766\n",
      "Epoch 169/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0699 - acc: 0.9779 - val_loss: 0.0796 - val_acc: 0.9765\n",
      "Epoch 170/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0696 - acc: 0.9783 - val_loss: 0.0799 - val_acc: 0.9766\n",
      "Epoch 171/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0706 - acc: 0.9783 - val_loss: 0.0797 - val_acc: 0.9768\n",
      "Epoch 172/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0712 - acc: 0.9774 - val_loss: 0.0796 - val_acc: 0.9764\n",
      "Epoch 173/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0702 - acc: 0.9782 - val_loss: 0.0797 - val_acc: 0.9763\n",
      "Epoch 174/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0680 - acc: 0.9785 - val_loss: 0.0795 - val_acc: 0.9768\n",
      "Epoch 175/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0701 - acc: 0.9779 - val_loss: 0.0793 - val_acc: 0.9768\n",
      "Epoch 176/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0674 - acc: 0.9788 - val_loss: 0.0798 - val_acc: 0.9769\n",
      "Epoch 177/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0699 - acc: 0.9782 - val_loss: 0.0800 - val_acc: 0.9760\n",
      "Epoch 178/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0682 - acc: 0.9777 - val_loss: 0.0790 - val_acc: 0.9773\n",
      "Epoch 179/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0699 - acc: 0.9781 - val_loss: 0.0788 - val_acc: 0.9771\n",
      "Epoch 180/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0676 - acc: 0.9785 - val_loss: 0.0790 - val_acc: 0.9763\n",
      "Epoch 181/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0683 - acc: 0.9789 - val_loss: 0.0794 - val_acc: 0.9764\n",
      "Epoch 182/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0678 - acc: 0.9778 - val_loss: 0.0786 - val_acc: 0.9769\n",
      "Epoch 183/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0680 - acc: 0.9785 - val_loss: 0.0791 - val_acc: 0.9769\n",
      "Epoch 184/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0657 - acc: 0.9795 - val_loss: 0.0788 - val_acc: 0.9767\n",
      "Epoch 185/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0662 - acc: 0.9797 - val_loss: 0.0791 - val_acc: 0.9770\n",
      "Epoch 186/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0661 - acc: 0.9795 - val_loss: 0.0788 - val_acc: 0.9767\n",
      "Epoch 187/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0671 - acc: 0.9792 - val_loss: 0.0793 - val_acc: 0.9770\n",
      "Epoch 188/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0676 - acc: 0.9788 - val_loss: 0.0789 - val_acc: 0.9768\n",
      "Epoch 189/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0651 - acc: 0.9791 - val_loss: 0.0783 - val_acc: 0.9770\n",
      "Epoch 190/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0649 - acc: 0.9798 - val_loss: 0.0789 - val_acc: 0.9767\n",
      "Epoch 191/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0667 - acc: 0.9788 - val_loss: 0.0787 - val_acc: 0.9772\n",
      "Epoch 192/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0666 - acc: 0.9795 - val_loss: 0.0791 - val_acc: 0.9771\n",
      "Epoch 193/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0640 - acc: 0.9804 - val_loss: 0.0792 - val_acc: 0.9771\n",
      "Epoch 194/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0660 - acc: 0.9794 - val_loss: 0.0787 - val_acc: 0.9770\n",
      "Epoch 195/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0653 - acc: 0.9798 - val_loss: 0.0788 - val_acc: 0.9768\n",
      "Epoch 196/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0649 - acc: 0.9795 - val_loss: 0.0788 - val_acc: 0.9769\n",
      "Epoch 197/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0630 - acc: 0.9800 - val_loss: 0.0785 - val_acc: 0.9769\n",
      "Epoch 198/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0640 - acc: 0.9797 - val_loss: 0.0790 - val_acc: 0.9771\n",
      "Epoch 199/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0635 - acc: 0.9804 - val_loss: 0.0791 - val_acc: 0.9773\n",
      "Epoch 200/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0631 - acc: 0.9803 - val_loss: 0.0786 - val_acc: 0.9772\n",
      " 9728/10000 [============================>.] - ETA: 0s\n",
      "Test score: 0.07376105632218533\n",
      "Test accuracy: 0.9792\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=BATCH_SIZE, epochs=200,\n",
    "                    verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"\\nTest score: {}\\nTest accuracy: {}\".format(score[0], score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интересно понаблюдать за тем, как возрастает верность на обучающем и тестовом наборе при увеличении числа периодов. На графике ниже, эти две кривые сходятся, когда число периодов примерно равно N, поэтому последующее обучение **ничего не даст**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cb8e712898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cb8046f438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Замечено, что сети со случайным прореживанием внутренних слоев часто лучше обобщаются на новые примеры из тестового набора. \n",
    "\n",
    "Интуитивно это можно объяснить тем, что каждый нейрон становится \"умнее\", потому что знает, что нельзя полагаться на соседей В процессе тестирования прореживание не производится, то есть используются все тщательно настроенные нейроны.\n",
    "В общем случае, рекомендуется проверять, как будет работать сеть, если применена та или иная форма прореживания.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестирование различных оптимизаторов в Keras\n",
    "\n",
    "Теперь попытаемся интуитивно понять процесс обучения сети. Один из самых популярных методов обучения является **метод градиентного спуска (gradient descent).** \n",
    "Представим себе функцию стоимости $C(w)$ от одной переменной $w$ с графиком сл. вида:\n",
    "![](https://www.safaribooksonline.com/library/view/deep-learning-with/9781787128422/assets/B06258_01_18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Простая аналогия для понимания градиентного спуска: альпинист, спускающийся с горы в долину. Гора представлена функцией $C$, а долина - минимальным значением $C_{min}$ Альпинист находится в начальной точки $w_0$ и передвигается небольшими шагами.\n",
    "\n",
    "На каждом шаге $r$ градиент дает направление максимального роста. Математически, это направление определяется частной производной $\\frac{\\partial c}{\\partial w}$ в точке $w_r$, в которой альпинист оказался на шаге $r$. Поэтому, двигаясь в противоположном направлении $-\\frac{\\partial c}{\\partial w}$, альпинист будет направляться в сторону долины. На каждом шаге альпиист может учитывать длину своей ноги пред следующим шагом. В терминологии GD это называется скоростью обучения $\\alpha \\geq 0$. Если она слишком мала, альпинист будет двигаться медленно, а если слишком велика, есть шанс проскочить мимо долины.\n",
    "\n",
    "Сигмоида - гладкая функция и вычислить её производную не должно составить проблем.\n",
    "\n",
    "Функция ReLU не дифференцируема в точке 0. Однако, можно доопределить ее производную в этой точке, выбрав в качестве значения 0 или 1. Тогда производной блока линейной ректификации $y=max(0,x)$ будет такая *кусочно-постоянная функция*:\n",
    "$$\\frac{\\partial y}{\\partial x} = \\begin{cases} 0, & x \\leq 0  \\\\ 1, & x > 0 \\end{cases}$$\n",
    "\n",
    "Зная производную, можно оптимизировать сеть методом градиентного спуска. Keras для вычислений использует базовую библиотеку (TensorFlow или Theano), так что нам не нужно думать о реализации.\n",
    "\n",
    "Нейронная сеть представляет собой композицию нескольких функций с тысячами (а иногда и миллионами) параметров. Каждый слой вычисляет функцию, ошибку которой необходимо минимизировать, чтобы улучшить верность на этапе обучения.\n",
    "\n",
    "\n",
    "В Keras реализован быстрый вариант градиентного спуска - **стозастический градиентный спуск (SGD)** и два более продвинутых метода оптимизхации: **RMSProp** и **Adam**. В обоих методах участвует понятие импульса к ускорению, используемого в GD. В результате достигается более быстрая сходимость, но ценой увеличения объёма вычислений.\n",
    "\n",
    "Полный список оптимизаторов в Keras: https://keras.io/optimizers/\n",
    "\n",
    "По умолчанию выбирается метод SGD. Чтобы выбрать другой метод оптимизации надо поменять пару строк:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop, Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OPTIMIZER = RMSprop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0955 - acc: 0.9695 - val_loss: 0.0877 - val_acc: 0.9756\n",
      "Epoch 2/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0914 - acc: 0.9716 - val_loss: 0.0868 - val_acc: 0.9772\n",
      "Epoch 3/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0878 - acc: 0.9717 - val_loss: 0.0896 - val_acc: 0.9752\n",
      "Epoch 4/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0835 - acc: 0.9734 - val_loss: 0.0895 - val_acc: 0.9770\n",
      "Epoch 5/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0813 - acc: 0.9749 - val_loss: 0.0940 - val_acc: 0.9763\n",
      "Epoch 6/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0778 - acc: 0.9756 - val_loss: 0.0890 - val_acc: 0.9788\n",
      "Epoch 7/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0779 - acc: 0.9767 - val_loss: 0.0904 - val_acc: 0.9780\n",
      "Epoch 8/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0712 - acc: 0.9779 - val_loss: 0.0942 - val_acc: 0.9782\n",
      "Epoch 9/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0725 - acc: 0.9777 - val_loss: 0.0913 - val_acc: 0.9767\n",
      "Epoch 10/200\n",
      "48000/48000 [==============================] - 5s - loss: 0.0683 - acc: 0.9788 - val_loss: 0.0962 - val_acc: 0.9786\n",
      "Epoch 11/200\n",
      "48000/48000 [==============================] - 5s - loss: 0.0692 - acc: 0.9790 - val_loss: 0.0982 - val_acc: 0.9776\n",
      "Epoch 12/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0664 - acc: 0.9803 - val_loss: 0.1008 - val_acc: 0.9778\n",
      "Epoch 13/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0661 - acc: 0.9805 - val_loss: 0.0958 - val_acc: 0.9768\n",
      "Epoch 14/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0642 - acc: 0.9810 - val_loss: 0.0955 - val_acc: 0.9779\n",
      "Epoch 15/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0641 - acc: 0.9807 - val_loss: 0.1011 - val_acc: 0.9774\n",
      "Epoch 16/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0623 - acc: 0.9813 - val_loss: 0.1000 - val_acc: 0.9780\n",
      "Epoch 17/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0607 - acc: 0.9819 - val_loss: 0.0998 - val_acc: 0.9794\n",
      "Epoch 18/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0583 - acc: 0.9823 - val_loss: 0.1008 - val_acc: 0.9781\n",
      "Epoch 19/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0613 - acc: 0.9818 - val_loss: 0.1035 - val_acc: 0.9780\n",
      "Epoch 20/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0593 - acc: 0.9829 - val_loss: 0.1069 - val_acc: 0.9783\n",
      "Epoch 21/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0590 - acc: 0.9827 - val_loss: 0.1038 - val_acc: 0.9788\n",
      "Epoch 22/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0609 - acc: 0.9830 - val_loss: 0.1054 - val_acc: 0.9791\n",
      "Epoch 23/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0577 - acc: 0.9836 - val_loss: 0.1036 - val_acc: 0.9802\n",
      "Epoch 24/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0584 - acc: 0.9835 - val_loss: 0.1077 - val_acc: 0.9790\n",
      "Epoch 25/200\n",
      "48000/48000 [==============================] - 5s - loss: 0.0584 - acc: 0.9832 - val_loss: 0.1078 - val_acc: 0.9792\n",
      "Epoch 26/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0574 - acc: 0.9838 - val_loss: 0.1087 - val_acc: 0.9795\n",
      "Epoch 27/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0595 - acc: 0.9836 - val_loss: 0.1133 - val_acc: 0.9787\n",
      "Epoch 28/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0539 - acc: 0.9848 - val_loss: 0.1119 - val_acc: 0.9788\n",
      "Epoch 29/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0536 - acc: 0.9841 - val_loss: 0.1196 - val_acc: 0.9778\n",
      "Epoch 30/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0563 - acc: 0.9843 - val_loss: 0.1149 - val_acc: 0.9785\n",
      "Epoch 31/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0563 - acc: 0.9845 - val_loss: 0.1237 - val_acc: 0.9773\n",
      "Epoch 32/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0554 - acc: 0.9847 - val_loss: 0.1231 - val_acc: 0.9784\n",
      "Epoch 33/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0551 - acc: 0.9854 - val_loss: 0.1188 - val_acc: 0.9792\n",
      "Epoch 34/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0570 - acc: 0.9841 - val_loss: 0.1227 - val_acc: 0.9782\n",
      "Epoch 35/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0522 - acc: 0.9846 - val_loss: 0.1221 - val_acc: 0.9788\n",
      "Epoch 36/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0546 - acc: 0.9855 - val_loss: 0.1197 - val_acc: 0.9789\n",
      "Epoch 37/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0551 - acc: 0.9850 - val_loss: 0.1284 - val_acc: 0.9782\n",
      "Epoch 38/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0531 - acc: 0.9853 - val_loss: 0.1225 - val_acc: 0.9789\n",
      "Epoch 39/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0566 - acc: 0.9852 - val_loss: 0.1237 - val_acc: 0.9777\n",
      "Epoch 40/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0530 - acc: 0.9859 - val_loss: 0.1196 - val_acc: 0.9791\n",
      "Epoch 41/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0538 - acc: 0.9853 - val_loss: 0.1177 - val_acc: 0.9780\n",
      "Epoch 42/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0510 - acc: 0.9860 - val_loss: 0.1213 - val_acc: 0.9778\n",
      "Epoch 43/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0508 - acc: 0.9860 - val_loss: 0.1247 - val_acc: 0.9789\n",
      "Epoch 44/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0485 - acc: 0.9862 - val_loss: 0.1232 - val_acc: 0.9800\n",
      "Epoch 45/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0545 - acc: 0.9858 - val_loss: 0.1241 - val_acc: 0.9783\n",
      "Epoch 46/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0516 - acc: 0.9859 - val_loss: 0.1313 - val_acc: 0.9771\n",
      "Epoch 47/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0483 - acc: 0.9869 - val_loss: 0.1316 - val_acc: 0.9782\n",
      "Epoch 48/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0523 - acc: 0.9866 - val_loss: 0.1320 - val_acc: 0.9787\n",
      "Epoch 49/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0507 - acc: 0.9863 - val_loss: 0.1301 - val_acc: 0.9778\n",
      "Epoch 50/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0520 - acc: 0.9869 - val_loss: 0.1338 - val_acc: 0.9778\n",
      "Epoch 51/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0501 - acc: 0.9865 - val_loss: 0.1378 - val_acc: 0.9779\n",
      "Epoch 52/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0509 - acc: 0.9862 - val_loss: 0.1361 - val_acc: 0.9777\n",
      "Epoch 53/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0504 - acc: 0.9871 - val_loss: 0.1432 - val_acc: 0.9780\n",
      "Epoch 54/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0531 - acc: 0.9867 - val_loss: 0.1331 - val_acc: 0.9782\n",
      "Epoch 55/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0516 - acc: 0.9871 - val_loss: 0.1403 - val_acc: 0.9772\n",
      "Epoch 56/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0485 - acc: 0.9873 - val_loss: 0.1418 - val_acc: 0.9772\n",
      "Epoch 57/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0479 - acc: 0.9873 - val_loss: 0.1455 - val_acc: 0.9777\n",
      "Epoch 58/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0531 - acc: 0.9873 - val_loss: 0.1422 - val_acc: 0.9778\n",
      "Epoch 59/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0536 - acc: 0.9866 - val_loss: 0.1374 - val_acc: 0.9778\n",
      "Epoch 60/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0544 - acc: 0.9869 - val_loss: 0.1314 - val_acc: 0.9787\n",
      "Epoch 61/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0521 - acc: 0.9880 - val_loss: 0.1439 - val_acc: 0.9790\n",
      "Epoch 62/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0506 - acc: 0.9870 - val_loss: 0.1499 - val_acc: 0.9761\n",
      "Epoch 63/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0500 - acc: 0.9878 - val_loss: 0.1427 - val_acc: 0.9776\n",
      "Epoch 64/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0506 - acc: 0.9871 - val_loss: 0.1460 - val_acc: 0.9764\n",
      "Epoch 65/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0536 - acc: 0.9860 - val_loss: 0.1487 - val_acc: 0.9768\n",
      "Epoch 66/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0530 - acc: 0.9871 - val_loss: 0.1449 - val_acc: 0.9780\n",
      "Epoch 67/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0545 - acc: 0.9869 - val_loss: 0.1471 - val_acc: 0.9783\n",
      "Epoch 68/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0515 - acc: 0.9871 - val_loss: 0.1507 - val_acc: 0.9776\n",
      "Epoch 69/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0494 - acc: 0.9880 - val_loss: 0.1431 - val_acc: 0.9771\n",
      "Epoch 70/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0514 - acc: 0.9873 - val_loss: 0.1432 - val_acc: 0.9790\n",
      "Epoch 71/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0533 - acc: 0.9869 - val_loss: 0.1425 - val_acc: 0.9780\n",
      "Epoch 72/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0524 - acc: 0.9881 - val_loss: 0.1502 - val_acc: 0.9773\n",
      "Epoch 73/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0514 - acc: 0.9878 - val_loss: 0.1454 - val_acc: 0.9774\n",
      "Epoch 74/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0521 - acc: 0.9873 - val_loss: 0.1443 - val_acc: 0.9787\n",
      "Epoch 75/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0522 - acc: 0.9880 - val_loss: 0.1445 - val_acc: 0.9777\n",
      "Epoch 76/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0535 - acc: 0.9875 - val_loss: 0.1448 - val_acc: 0.9783\n",
      "Epoch 77/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0522 - acc: 0.9884 - val_loss: 0.1566 - val_acc: 0.9770\n",
      "Epoch 78/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0485 - acc: 0.9879 - val_loss: 0.1452 - val_acc: 0.9787\n",
      "Epoch 79/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0517 - acc: 0.9875 - val_loss: 0.1459 - val_acc: 0.9792\n",
      "Epoch 80/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0557 - acc: 0.9879 - val_loss: 0.1496 - val_acc: 0.9784\n",
      "Epoch 81/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0509 - acc: 0.9884 - val_loss: 0.1459 - val_acc: 0.9782\n",
      "Epoch 82/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0532 - acc: 0.9882 - val_loss: 0.1425 - val_acc: 0.9792\n",
      "Epoch 83/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0528 - acc: 0.9879 - val_loss: 0.1427 - val_acc: 0.9775\n",
      "Epoch 84/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0531 - acc: 0.9877 - val_loss: 0.1462 - val_acc: 0.9788\n",
      "Epoch 85/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0537 - acc: 0.9876 - val_loss: 0.1413 - val_acc: 0.9778\n",
      "Epoch 86/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0498 - acc: 0.9886 - val_loss: 0.1561 - val_acc: 0.9783\n",
      "Epoch 87/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0492 - acc: 0.9886 - val_loss: 0.1587 - val_acc: 0.9773\n",
      "Epoch 88/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0523 - acc: 0.9873 - val_loss: 0.1529 - val_acc: 0.9777\n",
      "Epoch 89/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0483 - acc: 0.9885 - val_loss: 0.1528 - val_acc: 0.9798\n",
      "Epoch 90/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0512 - acc: 0.9881 - val_loss: 0.1516 - val_acc: 0.9784\n",
      "Epoch 91/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0515 - acc: 0.9879 - val_loss: 0.1673 - val_acc: 0.9773\n",
      "Epoch 92/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0501 - acc: 0.9879 - val_loss: 0.1539 - val_acc: 0.9777\n",
      "Epoch 93/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0510 - acc: 0.9881 - val_loss: 0.1592 - val_acc: 0.9787\n",
      "Epoch 94/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0556 - acc: 0.9872 - val_loss: 0.1500 - val_acc: 0.9788\n",
      "Epoch 95/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0500 - acc: 0.9885 - val_loss: 0.1569 - val_acc: 0.9780\n",
      "Epoch 96/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0520 - acc: 0.9888 - val_loss: 0.1495 - val_acc: 0.9796\n",
      "Epoch 97/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0493 - acc: 0.9890 - val_loss: 0.1507 - val_acc: 0.9775\n",
      "Epoch 98/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0516 - acc: 0.9881 - val_loss: 0.1548 - val_acc: 0.9788\n",
      "Epoch 99/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0481 - acc: 0.9886 - val_loss: 0.1534 - val_acc: 0.9787\n",
      "Epoch 100/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0528 - acc: 0.9881 - val_loss: 0.1558 - val_acc: 0.9773\n",
      "Epoch 101/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0541 - acc: 0.9878 - val_loss: 0.1503 - val_acc: 0.9789\n",
      "Epoch 102/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0502 - acc: 0.9894 - val_loss: 0.1568 - val_acc: 0.9791\n",
      "Epoch 103/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0530 - acc: 0.9884 - val_loss: 0.1567 - val_acc: 0.9783\n",
      "Epoch 104/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0507 - acc: 0.9887 - val_loss: 0.1482 - val_acc: 0.9778\n",
      "Epoch 105/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0539 - acc: 0.9876 - val_loss: 0.1601 - val_acc: 0.9779\n",
      "Epoch 106/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0520 - acc: 0.9884 - val_loss: 0.1597 - val_acc: 0.9783\n",
      "Epoch 107/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0524 - acc: 0.9886 - val_loss: 0.1555 - val_acc: 0.9777\n",
      "Epoch 108/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0531 - acc: 0.9884 - val_loss: 0.1560 - val_acc: 0.9784\n",
      "Epoch 109/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0569 - acc: 0.9875 - val_loss: 0.1643 - val_acc: 0.9768\n",
      "Epoch 110/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0509 - acc: 0.9889 - val_loss: 0.1659 - val_acc: 0.9777\n",
      "Epoch 111/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0536 - acc: 0.9879 - val_loss: 0.1562 - val_acc: 0.9780\n",
      "Epoch 112/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0485 - acc: 0.9895 - val_loss: 0.1688 - val_acc: 0.9772\n",
      "Epoch 113/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0502 - acc: 0.9889 - val_loss: 0.1606 - val_acc: 0.9771\n",
      "Epoch 114/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0560 - acc: 0.9882 - val_loss: 0.1652 - val_acc: 0.9771\n",
      "Epoch 115/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0481 - acc: 0.9894 - val_loss: 0.1762 - val_acc: 0.9775\n",
      "Epoch 116/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0538 - acc: 0.9889 - val_loss: 0.1629 - val_acc: 0.9780\n",
      "Epoch 117/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0503 - acc: 0.9886 - val_loss: 0.1771 - val_acc: 0.9766\n",
      "Epoch 118/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0509 - acc: 0.9881 - val_loss: 0.1690 - val_acc: 0.9775\n",
      "Epoch 119/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0509 - acc: 0.9892 - val_loss: 0.1684 - val_acc: 0.9770\n",
      "Epoch 120/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0496 - acc: 0.9892 - val_loss: 0.1700 - val_acc: 0.9772\n",
      "Epoch 121/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0578 - acc: 0.9877 - val_loss: 0.1639 - val_acc: 0.9782\n",
      "Epoch 122/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0533 - acc: 0.9889 - val_loss: 0.1621 - val_acc: 0.9783\n",
      "Epoch 123/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0503 - acc: 0.9893 - val_loss: 0.1654 - val_acc: 0.9780\n",
      "Epoch 124/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0533 - acc: 0.9886 - val_loss: 0.1618 - val_acc: 0.9775\n",
      "Epoch 125/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0475 - acc: 0.9895 - val_loss: 0.1624 - val_acc: 0.9788\n",
      "Epoch 126/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0508 - acc: 0.9881 - val_loss: 0.1617 - val_acc: 0.9783\n",
      "Epoch 127/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 3s - loss: 0.0512 - acc: 0.9891 - val_loss: 0.1583 - val_acc: 0.9798\n",
      "Epoch 128/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0537 - acc: 0.9888 - val_loss: 0.1621 - val_acc: 0.9789\n",
      "Epoch 129/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0523 - acc: 0.9886 - val_loss: 0.1637 - val_acc: 0.9790\n",
      "Epoch 130/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0581 - acc: 0.9883 - val_loss: 0.1687 - val_acc: 0.9785\n",
      "Epoch 131/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0476 - acc: 0.9898 - val_loss: 0.1736 - val_acc: 0.9784\n",
      "Epoch 132/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0495 - acc: 0.9890 - val_loss: 0.1651 - val_acc: 0.9790\n",
      "Epoch 133/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0509 - acc: 0.9890 - val_loss: 0.1675 - val_acc: 0.9785\n",
      "Epoch 134/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0509 - acc: 0.9892 - val_loss: 0.1677 - val_acc: 0.9786\n",
      "Epoch 135/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0516 - acc: 0.9889 - val_loss: 0.1659 - val_acc: 0.9788\n",
      "Epoch 136/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0553 - acc: 0.9887 - val_loss: 0.1658 - val_acc: 0.9788\n",
      "Epoch 137/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0548 - acc: 0.9889 - val_loss: 0.1724 - val_acc: 0.9779\n",
      "Epoch 138/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0568 - acc: 0.9885 - val_loss: 0.1677 - val_acc: 0.9783\n",
      "Epoch 139/200\n",
      "48000/48000 [==============================] - 5s - loss: 0.0511 - acc: 0.9891 - val_loss: 0.1713 - val_acc: 0.9791\n",
      "Epoch 140/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0499 - acc: 0.9891 - val_loss: 0.1577 - val_acc: 0.9792\n",
      "Epoch 141/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0547 - acc: 0.9889 - val_loss: 0.1712 - val_acc: 0.9780\n",
      "Epoch 142/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0535 - acc: 0.9885 - val_loss: 0.1650 - val_acc: 0.9785\n",
      "Epoch 143/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0557 - acc: 0.9892 - val_loss: 0.1612 - val_acc: 0.9796\n",
      "Epoch 144/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0526 - acc: 0.9884 - val_loss: 0.1677 - val_acc: 0.9792\n",
      "Epoch 145/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0517 - acc: 0.9886 - val_loss: 0.1755 - val_acc: 0.9775\n",
      "Epoch 146/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0501 - acc: 0.9897 - val_loss: 0.1653 - val_acc: 0.9779\n",
      "Epoch 147/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0513 - acc: 0.9895 - val_loss: 0.1663 - val_acc: 0.9787\n",
      "Epoch 148/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0495 - acc: 0.9896 - val_loss: 0.1797 - val_acc: 0.9773\n",
      "Epoch 149/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0500 - acc: 0.9901 - val_loss: 0.1738 - val_acc: 0.9777\n",
      "Epoch 150/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0539 - acc: 0.9884 - val_loss: 0.1776 - val_acc: 0.9775\n",
      "Epoch 151/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0494 - acc: 0.9893 - val_loss: 0.1731 - val_acc: 0.9782\n",
      "Epoch 152/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0521 - acc: 0.9895 - val_loss: 0.1847 - val_acc: 0.9787\n",
      "Epoch 153/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0564 - acc: 0.9893 - val_loss: 0.1682 - val_acc: 0.9782\n",
      "Epoch 154/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0572 - acc: 0.9882 - val_loss: 0.1720 - val_acc: 0.9784\n",
      "Epoch 155/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0527 - acc: 0.9895 - val_loss: 0.1732 - val_acc: 0.9779\n",
      "Epoch 156/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0508 - acc: 0.9898 - val_loss: 0.1770 - val_acc: 0.9779\n",
      "Epoch 157/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0521 - acc: 0.9898 - val_loss: 0.1732 - val_acc: 0.9787\n",
      "Epoch 158/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0532 - acc: 0.9891 - val_loss: 0.1737 - val_acc: 0.9777\n",
      "Epoch 159/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0508 - acc: 0.9897 - val_loss: 0.1785 - val_acc: 0.9779\n",
      "Epoch 160/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0549 - acc: 0.9892 - val_loss: 0.1750 - val_acc: 0.9784\n",
      "Epoch 161/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0522 - acc: 0.9897 - val_loss: 0.1769 - val_acc: 0.9777\n",
      "Epoch 162/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0567 - acc: 0.9890 - val_loss: 0.1854 - val_acc: 0.9776\n",
      "Epoch 163/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0541 - acc: 0.9893 - val_loss: 0.1768 - val_acc: 0.9774\n",
      "Epoch 164/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0501 - acc: 0.9895 - val_loss: 0.1739 - val_acc: 0.9776\n",
      "Epoch 165/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0547 - acc: 0.9893 - val_loss: 0.1855 - val_acc: 0.9782\n",
      "Epoch 166/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0569 - acc: 0.9890 - val_loss: 0.1735 - val_acc: 0.9781\n",
      "Epoch 167/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0543 - acc: 0.9895 - val_loss: 0.1726 - val_acc: 0.9792\n",
      "Epoch 168/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0542 - acc: 0.9889 - val_loss: 0.1802 - val_acc: 0.9783\n",
      "Epoch 169/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0577 - acc: 0.9889 - val_loss: 0.1748 - val_acc: 0.9785\n",
      "Epoch 170/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0534 - acc: 0.9889 - val_loss: 0.1760 - val_acc: 0.9778\n",
      "Epoch 171/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0526 - acc: 0.9896 - val_loss: 0.1831 - val_acc: 0.9773\n",
      "Epoch 172/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0569 - acc: 0.9889 - val_loss: 0.1857 - val_acc: 0.9768\n",
      "Epoch 173/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0601 - acc: 0.9894 - val_loss: 0.1802 - val_acc: 0.9774\n",
      "Epoch 174/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0500 - acc: 0.9898 - val_loss: 0.1884 - val_acc: 0.9772\n",
      "Epoch 175/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0606 - acc: 0.9891 - val_loss: 0.1788 - val_acc: 0.9773\n",
      "Epoch 176/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0553 - acc: 0.9894 - val_loss: 0.1792 - val_acc: 0.9775\n",
      "Epoch 177/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0530 - acc: 0.9895 - val_loss: 0.1882 - val_acc: 0.9783\n",
      "Epoch 178/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0512 - acc: 0.9904 - val_loss: 0.1858 - val_acc: 0.9779\n",
      "Epoch 179/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0539 - acc: 0.9891 - val_loss: 0.1830 - val_acc: 0.9784\n",
      "Epoch 180/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0566 - acc: 0.9887 - val_loss: 0.1866 - val_acc: 0.9778\n",
      "Epoch 181/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0576 - acc: 0.9891 - val_loss: 0.1775 - val_acc: 0.9781\n",
      "Epoch 182/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0490 - acc: 0.9902 - val_loss: 0.1781 - val_acc: 0.9777\n",
      "Epoch 183/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0548 - acc: 0.9895 - val_loss: 0.1799 - val_acc: 0.9782\n",
      "Epoch 184/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0550 - acc: 0.9891 - val_loss: 0.1729 - val_acc: 0.9787\n",
      "Epoch 185/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0484 - acc: 0.9903 - val_loss: 0.1767 - val_acc: 0.9787\n",
      "Epoch 186/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0538 - acc: 0.9900 - val_loss: 0.1832 - val_acc: 0.9773\n",
      "Epoch 187/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0530 - acc: 0.9900 - val_loss: 0.1806 - val_acc: 0.9789\n",
      "Epoch 188/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0539 - acc: 0.9902 - val_loss: 0.1806 - val_acc: 0.9775\n",
      "Epoch 189/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0568 - acc: 0.9897 - val_loss: 0.1902 - val_acc: 0.9778\n",
      "Epoch 190/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0565 - acc: 0.9894 - val_loss: 0.1792 - val_acc: 0.9765\n",
      "Epoch 191/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0543 - acc: 0.9899 - val_loss: 0.1838 - val_acc: 0.9781\n",
      "Epoch 192/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.0543 - acc: 0.9890 - val_loss: 0.1789 - val_acc: 0.9779\n",
      "Epoch 193/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0552 - acc: 0.9897 - val_loss: 0.1889 - val_acc: 0.9773\n",
      "Epoch 194/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0600 - acc: 0.9890 - val_loss: 0.1825 - val_acc: 0.9774\n",
      "Epoch 195/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0503 - acc: 0.9903 - val_loss: 0.1894 - val_acc: 0.9784\n",
      "Epoch 196/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0579 - acc: 0.9893 - val_loss: 0.1762 - val_acc: 0.9782\n",
      "Epoch 197/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0591 - acc: 0.9896 - val_loss: 0.1784 - val_acc: 0.9781\n",
      "Epoch 198/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0548 - acc: 0.9895 - val_loss: 0.1821 - val_acc: 0.9780\n",
      "Epoch 199/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0538 - acc: 0.9894 - val_loss: 0.1863 - val_acc: 0.9786\n",
      "Epoch 200/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.0591 - acc: 0.9890 - val_loss: 0.1883 - val_acc: 0.9769\n",
      " 9632/10000 [===========================>..] - ETA: 0s\n",
      "Test score: 0.1829059545627194\n",
      "Test accuracy: 0.9794\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=OPTIMIZER,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=BATCH_SIZE, epochs=200,\n",
    "                    verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"\\nTest score: {}\\nTest accuracy: {}\".format(score[0], score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLExIDMMwjGHx/wHYzcBY/Jxq4wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cb812a45f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzsnXeYVeW1/z9rKgwz1Bl6G3pTRIogdiygRqPGGksSozGJ\nGpOoMcWUm5vE+0sz5toj3hiN3RhQEDRiQwSG3mFmmGF67728vz/WOZwzBTgDc6axPs8zzz5n73fv\n/c6g+7tXedcS5xyGYRiGcSxCOnsChmEYRvfABMMwDMMICBMMwzAMIyBMMAzDMIyAMMEwDMMwAsIE\nwzAMwwgIEwzDaAdE5P9E5L8DHJsiIhee6HUMo6MxwTAMwzACwgTDMAzDCAgTDOOkweMKekBEtotI\nhYg8JyJDRGSliJSJyAciMsBv/BUisktEikXkIxGZ6ndslohs9pz3KtCr2b0uF5GtnnM/F5FTj3PO\nd4hIoogUisgyERnu2S8i8mcRyRWRUhHZISIzPMcuFZHdnrlliMj9x/UHM4xmmGAYJxvXABcBk4Av\nASuBnwBx6P8P9wKIyCTgZeA+z7EVwHIRiRCRCOBt4B/AQOB1z3XxnDsLWAp8CxgEPA0sE5HItkxU\nRC4AfgdcBwwDUoFXPIcvBs7x/B79PGMKPMeeA77lnIsBZgAftuW+hnEkTDCMk42/OudynHMZwKfA\neufcFudcNfAvYJZn3PXAu865951zdcAfgN7AmcB8IBx41DlX55x7A9jod487gaedc+udcw3Oub8D\nNZ7z2sJXgaXOuc3OuRrgx8ACERkL1AExwBRAnHN7nHNZnvPqgGki0tc5V+Sc29zG+xpGq5hgGCcb\nOX6fq1r5Hu35PBx9owfAOdcIpAEjPMcyXNPKnal+n8cAP/S4o4pFpBgY5TmvLTSfQzlqRYxwzn0I\n/C/wOJArIs+ISF/P0GuAS4FUEflYRBa08b6G0SomGIbROpnogx/QmAH60M8AsoARnn1eRvt9TgN+\n45zr7/cT5Zx7+QTn0Ad1cWUAOOcec87NBqahrqkHPPs3OueuBAajrrPX2nhfw2gVEwzDaJ3XgMtE\nZJGIhAM/RN1KnwPrgHrgXhEJF5GrgXl+5z4L3CUiZ3iC031E5DIRiWnjHF4Gvi4ip3niH79FXWgp\nIjLXc/1woAKoBho9MZavikg/jyutFGg8gb+DYRzGBMMwWsE5tw+4GfgrkI8GyL/knKt1ztUCVwNf\nAwrReMdbfucmAHegLqMiINEztq1z+AB4GHgTtWrGAzd4DvdFhakIdVsVAL/3HLsFSBGRUuAuNBZi\nGCeMWAMlwzAMIxDMwjAMwzACwgTDMAzDCAgTDMMwDCMgTDAMwzCMgAjr7Am0J7GxsW7s2LGdPQ3D\nMIxuw6ZNm/Kdc3GBjO1RgjF27FgSEhI6exqGYRjdBhFJPfYoxVxShmEYRkCYYBiGYRgBYYJhGIZh\nBESPimG0Rl1dHenp6VRXV3f2VIJKr169GDlyJOHh4Z09FcMweig9XjDS09OJiYlh7NixNC0u2nNw\nzlFQUEB6ejrx8fGdPR3DMHooPd4lVV1dzaBBg3qsWACICIMGDerxVpRhGJ1LjxcMoEeLhZeT4Xc0\nDKNzOSkEwzAMo0tSWQg73ujsWQSMCUaQKS4u5oknnmjzeZdeeinFxcVBmJFhGF2GrS/Bm7dDaebR\nx2Vthy7QisIEI8gcSTDq6+uPet6KFSvo379/sKZlGEZXoDhNt6VZTffXVsDav0BDHRz6Ap4+G9I2\ntH6NDhQSE4wg89BDD5GUlMRpp53G3LlzOfvss7niiiuYNm0aAF/+8peZPXs206dP55lnnjl83tix\nY8nPzyclJYWpU6dyxx13MH36dC6++GKqqqo669cxjJOPzK3w9DlQXerbt2c57H33xK9dmqHbsmaC\nsf89eP/nkPKpWhcAxYdav8bGv8HrX4PayhOfzzHo8Wm1/vxq+S52Z5Yee2AbmDa8L7/40vQjHn/k\nkUfYuXMnW7du5aOPPuKyyy5j586dh9Nfly5dysCBA6mqqmLu3Llcc801DBo0qMk1Dhw4wMsvv8yz\nzz7Lddddx5tvvsnNN9/crr+HYRhHIPkjyNoGeXthlKd1+5rf6sP++7shMvr4r12Srtvy7Gb7PUKS\nuwcKk/VzZX7L8ysLYc1vYOgpEN77+OcRIGZhdDDz5s1rslbiscceY+bMmcyfP5+0tDQOHDjQ4pz4\n+HhOO+00AGbPnk1KSkpHTdcwjKKDui3xuI+c07f96hLY/ILua2yEba9AXRtT272CUdZMMLwxjdzd\nkLdPP1e0Ihgf/4/OY/Ej0AGZkieVhXE0S6Cj6NOnz+HPH330ER988AHr1q0jKiqK8847r9W1FJGR\nkYc/h4aGmkvKMIJNQz0UJkHcZChK0X3et/6qIqgtBwTWPQ7z7oCkNfCvb0FIGJzylcDuUVftsxqa\nu6S8rqqc3T5RqchrOqYsGzY8C7O/BkM65tlmFkaQiYmJoaysrNVjJSUlDBgwgKioKPbu3csXX3zR\nwbMzjB7M+qfhk9/7vteUwRNnQnoALRA++zM8sQDKcqDQa2F4HtzeWMKsr0JpOuxZBnvf8RwLuFK4\nTxRA79PasZydUJGrnysLmo5J/ghcA8z+euD3PEFMMILMoEGDWLhwITNmzOCBBx5ocmzx4sXU19cz\ndepUHnroIebPn99JszSMLohz8MpXIfGD4zt/60uw+R++7wWJkLsL0tYf/byGOkh4Th/Gaet9QuF9\niHtdU3Nuh/6jIeF5DVKDL+spELzXi4hu3SUVGgkNtfpdQlq6pJI/ht4DYciMwO95gpxULqnO4p//\n/Ger+yMjI1m5cmWrx7xxitjYWHbu3Hl4//3339/u8zOMLklVkb65DxwHEy5s+/lFqWpVNNRDaBiU\ne97Uy3OOft6+lT4X0Z7lKhzgEwqvhTFgLJx+G3z4a9+5JW0QDK+La/gsDW5Xl+givlm3qICMvwCS\n/qNjhs1s6pJyDg5+AvFnQ0jHvfebhWEYRtfE+2CvPo4FrNUlep5rgDJPANn7Fl+ed+TzQNNU+46E\n2EkqHqCfvQ/44jQI7wO9B+jDPSQMJBTGnn10C8M5za7K3KrfSz2Wy4jZGsvY+By8+wPYuxxwMGGR\nHg+N9I3xUpis58efG/CfpD0wwTAMo2viFYyq4xCMotSWn70Whjcm0Br1NZDymQauR8yGWk/8cexZ\n+sCuq1ILo/9ozUqKGaJWxoxrNLW1JO3IC+nKczWradVP9XtJOkQNgoGerMld/2q6jZ0E/cfots9g\nFcF6j4vq4Me67UmCISKLRWSfiCSKyEOtHJ8iIutEpEZE7m927PsisktEdorIyyLSK5hzNQyji1F2\nAhaGN7MJfC4krwAdzSVVeFCtkiEzYJimshPWC0bM0c+lmVByCPqP8p1z+Z/gmmeh3yioq9S1Ea2R\nu1u3qZ/pYrySDOg3EqKH6v5szwK9A+/rtu9wOOcBWHgv9InVfd7Ad9IaiBkOg8Yf9c/Q3gRNMEQk\nFHgcWAJMA24UkWnNhhUC9wJ/aHbuCM/+Oc65GUAocEOw5moYRhfkRCwM/2wl7+fyAFxS+ft1GzsR\nhnsEo/8Yn0CUpPksjOYcHnOEFdm5e3QbGgmfP6bz6jsSYob6xkTEQL0ntb7vcDj9Fjj1Oj/ByFcr\nI2kNTLyoQ9Ze+BNMC2MekOicS3bO1QKvAFf6D3DO5TrnNgJ1rZwfBvQWkTAgCjhGdS7DMHoUJxLD\nKEqFyH76QD5sYXhdUnm60A70Tf+Lp3zneQVj0AR1MUmIuoz6jdT9uXvVNdTPz8Lw4t3njWPUVTXN\nfsrbA1GxMOtm2PG63mvAGIgZ5hszx5MiG94HevnVkusT55l7Phz6XF1lkxa37W/SDgRTMEYA/hGg\ndM++Y+Kcy0CtjkNAFlDinFvd2lgRuVNEEkQkIS/vGMEswzC6HrUV8LcLIWNT0/3eB3xVSduvWZwK\nA0brA9kbw/A+vF0DVHncRhv/Bu/9yGfF5O9XkYmMhog+MOcbMP1q6Ot5dB1ap9tWLQzPPm8c47Xb\n9PfykrsHBk+FC38J1zwHVz4OC+9T60FCVHBmXKNj+w5vaj1EeSyMinzYv0qtlHEdG7+ALhr0FpEB\nqDUSDwwH+ohIq8WTnHPPOOfmOOfmxMXFdeQ0A+J4y5sDPProo1RWBr+gmGF0KoXJkL7Rl5HkxWth\n1JRAY8PRr+Gcb4EdqEgMGKsP8eJDerw812cFeMWoIEm3Wdt0m79f3VFeLvsjzLwewiI18Lz737p/\nYCutkHsPUMugOE3XZRxYpeJRWaj3z90Dg6dBr74aVJ91swbNQ0JhQLwKwJAZEB6lguGPv0tq30qI\nP0cFrYMJpmBkAP5220jPvkC4EDjonMtzztUBbwFntvP8OgQTDMM4Bt4Fadk7mu73D05XH8PKOPA+\nPHaaruJ2Ti2M/mP0pzRDH9r1Vepm8r92oUcwMrfoefkHNCupNU67Ud1A1/6fLyDuj4jGMTISYOWP\nNFgOKkIlaVpOZPDU1q9923K45He6XuTcBzVd159e/TV1N2mN1raadMnR/x5BIpgL9zYCE0UkHhWK\nG4CbAjz3EDBfRKKAKmAREMB6/q6Hf3nziy66iMGDB/Paa69RU1PDVVddxa9+9SsqKiq47rrrSE9P\np6GhgYcffpicnBwyMzM5//zziY2NZc2aNZ39qxhdmdoKzaBpzVXS1fFm/mTv1N9j6WJY9At9qEdE\n64O2uhiiBvrOqauG0HB9OwctkwGaktpvpAaOB4zVt3Wcz901ZAbsW6FxjNoK3wK9rK36uba8qYXh\nz0X/dezfpd8oSHxfg9eX/xne/rYKhlfwBjfP+/Ge5+etP+v7LY+HhGgK7oFVENnX57rqYIImGM65\nehG5G1iFZjktdc7tEpG7PMefEpGhqBD0BRpF5D5gmnNuvYi8AWwG6oEtwDOt3qgtrHyo5VvMiTL0\nFFjyyBEP+5c3X716NW+88QYbNmzAOccVV1zBJ598Ql5eHsOHD+fdd7W+fklJCf369eNPf/oTa9as\nITY2tn3nbPQ8Pv8rrH8KHkju0JW/7YLXwihNV5dP9nbY9rKu9B4+S9/+/TOlnNOGQhMuhMW/031p\nnjpse5bpmgnwCEbvpsf9LQxv2fCw3rqY7nCG1BEsjEA4824YOVcLEvbqB8vv0+t6RXHwlOO/dp84\nXUNyxl1NxbMDCWppEOfcCmBFs31P+X3ORl1VrZ37C+AXwZxfR7N69WpWr17NrFmzACgvL+fAgQOc\nffbZ/PCHP+RHP/oRl19+OWeffXYnz9ToFqx7QktH3Pym+umrivQtuV9AuSWdhzdDySts/iUvPvuz\nbr21mWInq2D4Z0rl7NSHcE0ZXPJbzUbK2qZv98WHYPn31NIae5bWYgrrDVte9FxvogaMy3N98YvJ\nS2DXW5D0oWfMCQjGuPP0x0vsRMjbD431MGiiisjxEh0HJX1h/reP/xonyMlVS+oolkBH4Jzjxz/+\nMd/61rdaHNu8eTMrVqzgZz/7GYsWLeLnP/95J8zQ6FakrtVVyc753mCLDnZ9wVh2jwrADS/p98p8\nT6G9GhWCkHBP+XAgzvPw9rcw9q/SbVmWegxqSvWBfN6PYdndaj1c/5JaF+G9YfqX1WIBiB4C0YNV\nMLzxixlXq2Cs/YuW9/BfF3GixE7UFquVhWp1nAiLfqF/l06yLqCLZkn1JPzLm19yySUsXbqU8nL9\nnyEjI4Pc3FwyMzOJiori5ptv5oEHHmDz5s0tzjWMFpRlq6/eG7+ApplCXZW09b6sJFCX1MB4fZiD\nby0CQJzHheNvYRxYre4mUJ++N9V18hKYfKn+TLnMN/7023QbGqGZTNGD1bVTkKz3HLMQELUArnuh\nfRfDxU5SYWuogYkXn9i1Rpyu2VGdyMllYXQC/uXNlyxZwk033cSCBQsAiI6O5sUXXyQxMZEHHniA\nkJAQwsPDefLJJwG48847Wbx4McOHD7egt9ES77qCirymFkZ74RzsfFMzg06kDak/jQ2esh1OP4eE\n6tyjYjVYnZijaxO2v6qB4tjJep7Xwqgs1BTccx5QS2PPcrVI4qbqm/cNL+m8/R/6o+frdeoqdX+f\nwVrHqa4KBo7X8258WTOf2vvt3eveioiB0Qva99qdgAlGB9C8vPn3vve9Jt/Hjx/PJZe0TJO75557\nuOeee4I6N6Ob0tjoK3VRWRAcCyNrG7x5u2b7zPlG+1yzJB0aPYUdKvLU/VORD0OmwfSrdIV1vxEw\n+kzYv1JjEaERPgtj28vgGn1ppR//DyAay/DS3EIQ0UVy3ljJgLGayRQSDqd4so0mL2mf3685XsEY\nfz6ERQTnHh2ICYZhdEeqCtVvD+ry8KZtejN/2oO0DZ5rHkOE3v85jDtfH4rHwn9+pZkqGJX5amFM\nv0p/AM64Ux/sYR43UlUxHPgAVj8M4xfBsFm6Iru+Gk69QQXnaIya6/t87oNQcEAbM51IgDsQYidp\nL4vTbw3ufToIEwzD6I7494D2poOGRrSvSyrdIxhHaztaXarB4vwDKhilmbp2olff1sc3F4yhp2p2\nV59mqePjL9Af0EVrFfnw1jdVGK77u2ZYxQwJbG1Ec6IGwk2va/xjbJAzEsN7wbc+Ce49OpCTIujt\njlSfvgdxMvyOhh9Nitp5BGPoKWppHKm8dlvxWhhFRxGMvH26PbROW5s+cx6s+Y3ue+k6eO/HTccX\nHQQ8LqPSTF9Np6ijrDXq3V/7P1QVwbkPQWRMW3+TloSEqBuqvWIzJwk9XjB69epFQUFBj36gOuco\nKCigVy9rGXLS0JqFMWK2br1Wxs43fWsN2kp5rloWoRFHtzDyPCW7q4pg8wua0uqtDpu1VUtZ+FN4\n0LMWIkJLdngX7fUZdOR79Oqv6aShEU3XOBgdTo93SY0cOZL09HR6eiXbXr16MXJkq2sgjZ6I18KI\nHuInGHOAZ/ShXJ4Lb3xDaxJd+b9tv77Xuph4sfbVri5pfdFZ7l7UYnDwkWedU0W+b21IRb52sQuL\n1GOFyRrYrq9W0fO2HT2WhQGa/moWQafS4wUjPDyc+PhWKksaRneipgzeuhOW/I9mDpVla22hmGH6\nJg9qYYSEw8f/z+fqydl1fPdL36DXmnalCkZRKgw7teW4vD3qCivL9rU+rczXrCZvUD5vn57b2Khi\nNv4CtUhKM/0sjKMIhrcvxImuYzBOmB7vkjKMHkHWdi2al/gf/V6WrWLRx6+kf7+RcP0/dJFYdalm\nE+XuOXZp8NZI26jZPd5CfMWpajUUpTQNXHtLdo/xrDGI7AsVBfrjxSta5dlaMXbgOC3fXZrhSwfu\nc5TWBL0H6LaTKrQaPkwwDCMQKgth1U/VvdIZeNcQeHtVl2VpSqr3zTwiWjNyJi+B726E7+/Uiqb1\nVW1fm9FQB5mbYdQ834rq7B3w+Bnwl5nwxAIVjapincfgKWo1hEZqn4eakqYxlpydui1I1O3AcSp2\npVk+C6P3URbMzfoqXPHXDu9fbbTEBMMwAiF5Daz7Xy2E1xkcFgzPw78sG6KHqlsKmq5QDovQ8hdD\npuv3nDZWaM7erjGGkXP17T6yH6x7HPL3aT2jkHB494eQt1fHx02FWbfCfTu0fDjoWNDCf14LI9c7\nfop2sKuv0vUQvQdoH4gj0X90j1nH0N0xwTCMQKgq0q33jbgjKEiCF6/xxAf8LIzGBs1G8rcwWntD\nj5uirT/94xh11ce+b9pG3Y6ap9sBozVLafwiOPsHsOjnWtn1rTv1+OCpvnUR3vl4021Hz/cTjN0a\nj4gZ6usot2+lNjkyugUmGIYRCIcFo4Oy7Rrq4a07dDVy2npfS9HCFJ2Da/AIhsf3H9VKWmp4Ly2o\n531g73kH/mesL+31SKRvgJjhGhMB3wP97B/odu7tsOBuPT7tSl/bU/95eK2PcedqMLw81xfvEPH1\nyI6KhaueDvSvYnQyPT5LyjDahcPF7zrIwlj7qK9LXEmGT6hqSnzd5eKmaKVaaF0wQN1SGZ5mlVtf\nUjfQnndgwXeOfO+0jU1LaZxyrYrTmIX6PSQULvlN6+dG+VkYETEwar5+P7ROBeOUr+j34bO0JemM\na9QyMboFZmEYRiActjAKjj6uvdjxupayDuvtWeCWx+EV0ltf0jjCyDk+F9CRBGPoDLUocnb5Mqz2\nrWh9LEBxGpQcgpHzfPumfxku+2NgZb+98ynP0cV4I+doDGTzCyp23p7WoWEqWiYW3QoTDMMIBK9g\ndISF4Zyuexh6qlZuLUlXwfD2hjj4qfZGCO/tF/Q+gmCccp2Kzj+u1nTbMWdB6uea9dW8+oFzsOJ+\nzXY63uqtvQdwWNiiYrXv9oQL1LUGR+5pbXQLTDAMIxA6MoZRnquuo/5j1Ndfkg7lefq2DoDz9Vbo\n6ykFPuYIvRb6j4JzfqhrIGKGwYW/0PjHqzdrPGPtY751Gptf0NaoF/7y+FNYQ0J96ya81ob/gjuv\nhWF0S4IqGCKyWET2iUiiiDzUyvEpIrJORGpE5P5mx/qLyBsisldE9ohI9+8+YnRfOtIl5V1rMWCs\nBpYLk6C2TL/3GazHxpyp27AI+MZK7V99JM6811Ni+zYtHxIzTGMKA+Ph/Ye1BzZo6uyIOXDGXSc2\n/8NuMs92woW6jR7Sqe1FjRMnaEFvEQkFHgcuAtKBjSKyzDm3229YIXAv8OVWLvEX4D3n3FdEJAKI\nCtZcDeOYdKRLyl8w+o7w3btPnD7kK/Jg1BmBXy8ssmmJ7VveVktg0ATtgb3jDVjwXV07ccnvNEX2\nRIiKBfb7CgpGD1aL6GiL84xuQTCzpOYBic65ZAAReQW4EjgsGM65XCBXRC7zP1FE+gHnAF/zjKsF\naoM4V8M4Ms41XYfRvAXoibD6Z3rtuXfA8NN0n1cw+o/WGIaX6MFqSURE+wryHQ+Dp/g+n3o9bHkR\nVj6o39ujXpNXKPwLCt70qq4JMbo1wfwXHAGk+X1P9+wLhHggD3heRLaIyN9EpE9rA0XkThFJEJGE\nnl6R1ugk6iqhoVbf8BvrtP5RwlItoXGibFyqD+y/Xegr4VGUousgwnv51kKA3n/Rz+GWt078vl5G\nn6kB84OfwID49im/4Q3A+xcU7NWvffpYGJ1KV5X8MOB04Enn3CygAmgRAwFwzj3jnJvjnJsTF3eU\nAmaGEQipn8OLX4F6P4PWa10M8hTi2/IivPP9lr0e2kpNOdRVwGlfVSFK8qS9FqX4ajj1bSYY7U1o\nGEzxGPgTL24fyymqWQzD6DEEUzAyAL8loIz07AuEdCDdObfe8/0NVEAMI7is+S0kvu8rmAc+wfBW\nbj2wWrdHaywUCN5y4GMW6mpp74K84lQY4Fld7e+SCoZgAMzwLKabctnRxwWK17I4WlMko1sSTMHY\nCEwUkXhP0PoGYFkgJzrnsoE0EZns2bUIv9iHYQSFnF2Q8ql+zvYr2NdcMA59oduSdN1mbW+5pqE1\n6mt09XbxIR3vLfcRPQTiz9X1FbWV2ifCa2FExujCt4hoiAhS3se4c+HeLbptD4aeqjWjBlgfmp5G\n0ILezrl6EbkbWAWEAkudc7tE5C7P8adEZCiQAPQFGkXkPmCac64UuAd4ySM2ycDXgzVXo4fx8f/T\nhkMX/7pt5214FsJ6QUiYVmz10twl5TzrFkrSIHsnPH023PjKsRe7ffEkfPAL/XzhL2GgJ14QPVhb\nj259Efa+CzifYIBaGXWVbftd2srAce13rbEL4aETtL6MLklQa0k551YAK5rte8rvczbqqmrt3K3A\nnNaOGcZR2f+ethRtq2DsfAumX6UWQFYrguG1MLwUp/kskZxdrQvG7n/D3hVw9dPaGa/vCLUuMreq\n1QAqGNGeEhnvP6xb/wf48Fm+WlaG0YlY8UGj51Ge2/YHbF211jqKnagZPZv/oSugQ0J9ghEzTB/y\nteXaDrUkzVeV1dscCNSlVJKhD/rVP1MBuvjX2g9i2Eyoq9I4RcVkQDwlNMK0flPRQbjgYe1F4eWK\nvwbm8jKMIGOCYfQsnNPCdw21Wsk1otVs7JZ4e2D3HqiNieoqtKtc7EQVjNBIv9pNor0hPvl9y25y\nAKt+AruXaRlwbynx9I06ZvISvdfubZ4CfbG+5kFfX6HXbt5MKCT0eP8ahtGudNW0WsM4PqqLVSxA\nH8iBUukRjKiBMOxU/Zy1TbdVRVofSUSzl0af4clicpCyVsd4BaOxQdNtXQNseMbTS0Jg55uaOhs3\nRfdVFWqDJG+pD9BCfUfrPGcYnYwJhtGz8GYeAZS1QTD8LYzYyVo+/MP/hn9/V9dFeAvqXfMcXPWM\nr2lQfZW6sKqKtM5U5lYVrXMfUmG48JdagmPvuzp+8BRfQDtzi8YvDKObYIJh9Cz8rYry7MDP87cw\nwiLg3B9pgHrbq7oK2isY0YN1fUF/vyVGkxbrtiBRW5ciMO9O+N42mHG1lvyor9bSGLGTfGssast9\nwW7D6AaYYBg9C38Lw//zsfC3MADOfQC+/i58+Qn93rzKal+/BXXeBW9ewRg2U0XFu2p6mKdG1ICx\nGgfpP9Z3brRVJzC6D+YwNboumVs01fWi/wq8ZIW/hVF2nBaGP6dep9Ve+zbL/g6L1OB4eY4GwEPC\n1RJJ3wALv9d07LCZuo2b6ruHN9vKLAyjG2EWhtE5JH+s7p6j8dmj8PljLZsWHfoCHj219d4U5bkQ\nGqEpsP7i4a0yeySqiiC8jwpBc6ZdCSNnt9zff5RaDZHRWnZ8+yu68G/mjU3HDTtVBWXoKfpdxBMM\np2nQ2zC6OCYYRufw8f+D1T898vH6Gl8Pav+UVdASGsWpvjIe/pTn6lt7zFCfYBQehD9O0UylI1FZ\n2PbmPmd9H873/A6xk1QUrn+x5QK/Xv3gm+9rzwkv3jiGBb2NboQJhtHxOAc5O9Ry8C6wqy6BJxfC\ngff1e8qn2mUONP3Un0LPd29NJ3/Kczwrp4f6sqT2vqsprUcTjKpCX2A7UKZcBqdeq58v/m/4xnsw\n/vzWxw6fBb36+r57LQxzSRndCBMMo+MpSVeBAJ/1sP01XQS39SX9vneFuohCwn0C4cUrIIfWtby2\n18KIHux0cI70AAAgAElEQVTLktq3UrdJH+pivtaoLDix9qED4/16bgfAkGnqOus7/PjvaRgdjAmG\n0fH4lw7P368WR8JS/Z70ofai2LcSJlygrptWLQzROk415U2PledoGfCYoRq3qMhXYRl1hqa2Jn3Y\n+pwqCzu2hejMm+DujSfWOc8wOhgTDKPjyfYIhoRC/gF1LeXuhgkXqeWx+qdQlgmnXKuL3vwFo6pY\nrYH4c3Q1dUaC71hjg/bcjh7icfU42PpPHXfRf2nJ7b1NamHC5hd0YV7VccQwToTQsKYVaQ2jG2CC\nYQSHmnJY/3TrbUxzdujDcuA4KDigD+3IvlpkLyRMS2rETYEpX9IS4IXJ0Nio53rdUzNvAARS/dxS\nFfngGtUdFTNU9332JxWPkfN0gd3+ldBQr8cKkmDZPbD2MRWijrQwDKMbYoJhBIetL8HKB30lMfzJ\n3glDZmg2Uc4u2PsOTP0S9B0Go+brmHMegJAQGDROy2+UZen+gmTdDj8dRs+HL56AjM3aeMjbCS96\niC82ENYbvrJUrzXlMk2f9cY+vHNLfB9wHWthGEY3xBbuGcFhn8f1s+tfMP3Lvv21niqwp16nTYG8\n46Zfrdt534SoAdqXAnxNhgqTtJGQN34xYKzWdXp+CTx3sWZBgVoqQ6ar9XL9i9r+1CsE4y/QqrP7\nVkD82T7B8FaUNQvDMI6KCYZxYtRWaNvR+HN8+6pLtYpraIS+9fuXGc8/ADgYPFXHgaazetuDTr/K\nJxYAgzyCse89QDSrqt8oCO+lAnLbcu1kFzVQU1fHnq3HQK0WfyKjNe117ztw1g8gbb1aKpmb9bhZ\nGIZxVMwlZZwYCc/D36+Acr/V2En/0Tf+cx5QK8LrKgJffaeY4brYDfTBHhre+vX7joSIGPjicfj7\n5bDjDXVTeRkwBpY8Auc+CBMv8onFkZh8qVoUy+4GHCz6ue+YWRiGcVRMMIwTI28P4LT7nJd976nV\nsPA+LX2x+9++YxUewYiO05IZUy6HM+468vVDQuD2VXDbO3D5oxrQHrPw+Oc7+VKNaxxYrS6qcef5\n3F5RbVy4ZxgnGUF1SYnIYuAvQCjwN+fcI82OTwGeB04Hfuqc+0Oz46FAApDhnLs8mHM1jpP8A7ot\nzYQRp2sv7F1vaRZTWIS6gJI/0rUWIj4Lo0+cVm694aVj32PIdN3Gnw1zvn5i842Og+/vgogovT/A\nqHkaGzELwzCOStAsDM/D/nFgCTANuFFEpjUbVgjcC/yB1vkesCdYczTaAX/BqK2EN7+pD95Fv9T9\no87QxXRFKfq9Il9XcAfaOjUY9BnkEwuA02+DWTdrzSfDMI5IMF1S84BE51yyc64WeAW40n+Acy7X\nObcRaJGsLyIjgcuAvwVxjsaJUFHg6yNRmgG734b8fXDl4/pQBhUMgLQNnnNyu14PiDELdM6BllA3\njJOUYArGCMDPsU26Z1+gPAo8CDQebZCI3CkiCSKSkJeXd7ShRntTcMD3uTQTcvdo2qp/Ab7BUzXV\nNc1TKLAiT91RhmF0O7pk0FtELgdynXObjjXWOfeMc26Oc25OXJw9iDqU/P26jRmmgpF/QNNgQ0J9\nY0JCtSjfofX6vTzPekAYRjclmIKRAfg1PmakZ18gLASuEJEU1JV1gYi82L7TM46b5I/gtVs1wB0a\nqW6nskwVkEETWo4fNV9rRVWXdE2XlGEYARFMwdgITBSReBGJAG4AlgVyonPux865kc65sZ7zPnTO\n3Ry8qRptYuNzmiqbsFQtiv6joCRDA9vetRX+jJoHOEjfqIUDzcIwjG5J0NJqnXP1InI3sApNq13q\nnNslInd5jj8lIkPRtNm+QKOI3AdMc86VBmtexgnSUK/tVcN6abnw2InQdwQ01Ojx5t3mwNfTOvlj\nLQ5oMQzD6JYEdR2Gc24FsKLZvqf8PmejrqqjXeMj4KMgTM84HjI2QU2JVpbd8AyMO79pSY3WBCNq\noIpK0hr9bi4pw+iWWC0po20k/QckRFdon36r7kv360kxqBXBABh6Cux/Tz+bS8owuiVdMkvK6MIk\n/kcL9vlbFd5S4tFDmvat9mfIDN9nc0kZRrfEBMMInIoCrew6YVHT/dFDtHteawFvL0NP8RtvgmEY\n3RETDCNwDqzWoPWkxU33h4TC0Bm+Vd2t4RWMkHBtlWoYRrfDYhjGkcndAzvfhAPvwwUPa3vT6KEw\n7LSWY7/5ocY2jsSAeK0h1aufleAwjG6KCYbRktoKeP5SyNqqIhARA+9+HyqLYMbVWnK8OaHH+E8p\nJEStjIba4MzZMIygY4JhtCR7h4rFmffCgrshby+8cIUem7zk+K/7pb+YYBhGN8YEw2iJt2T5nK9D\nzBD9mXoFJH0I8ece/3UHT2mf+RmG0SmYYBgtyd+v/bj7j/Htu+ppKMvSxkOGYZyUWJZUT+ffd8OW\nNtZtLEjUtqX+VWcjorRulGEYJy0mGD2ZxgbY9gpsf61t5+Xvb73Eh2EYJzUmGD2ZsixorIOcndpT\nOxDqa6HwoAmGYRgtMMHoyRQf0m1lgYpHIBSlgGs4+qptwzBOSkwwejJewQBNlQ0Ebxc9szAMw2iG\nCUZPpijV9zl7+5HHbXsVvnhSP3v7dB+p6qxhGCctllbbncnZDf1GaLmN1ig+pKU8wntB9k4ozdLP\nvQf4xjQ2wvs/h/JsiBkKBz44etVZwzBOWszC6K5UFMDTZ8NfZsLmF1ofU5wKA8ZoSY6UT+GvszXN\n1p/MzSoWEdHw+tcg9TM454GgT98wjO5HQIIhIt8Tkb6iPCcim0Xk4mBPzjgKRQehsV5bpS7/HlQV\nw57l8ORCFZCGehWM/qNhyCka+K6r0NXa9TW+6+xZDiFhcOsyGDUfrvsHzLuj834vwzC6LIFaGN/w\n9Nm+GBgA3AI8cqyTRGSxiOwTkUQReaiV41NEZJ2I1IjI/X77R4nIGhHZLSK7ROR7Ac7z5KHYE584\n6wdacvzQOhWKnF2w7B54/2EoyVDBmHIpjL8AFv8P1FVC2gbfdfa+C2PPhpGz4fZVMO2Kzvl9DMPo\n8gQqGN561JcC/3DO7fLb1/oJIqHA48ASYBpwo4hMazasELgX+EOz/fXAD51z04D5wHdbOffkxpsB\nNeMaCI3UEuQHP1Xr4JRrYf3Tmh7bf7S6pG75F5x2k1oTSR/quXn7NMg95bLO+z0Mw+g2BCoYm0Rk\nNSoYq0QkBmg8xjnzgETnXLJzrhZ4BbjSf4BzLtc5txGoa7Y/yzm32fO5DNgDjAhwricHRakQNQj6\nDIJR87T8R30VTLgIFt6nYgFN60H16gsj52lfblB3FJhgGIYREIEKxu3AQ8Bc51wlEA58/RjnjADS\n/L6ncxwPfREZC8wC1rf13B5HQz188CvNdio+pNYDQPw50FCjlsbYhdr9brynjap3jJcJF0DWNijL\nUXfUiNm+ntyGYRhHIVDBWADsc84Vi8jNwM+AkuBNSxGRaOBN4D5PDKW1MXeKSIKIJOTl5QV7Sp1L\n9nb47E+w9cWmgjH2bN2OORMi+ujni/4L5twOA8Y2vcbUK7Up0soHNUPKrAvDMAIkUMF4EqgUkZnA\nD4Ek4Ai5nIfJAEb5fR/p2RcQIhKOisVLzrm3jjTOOfeMc26Oc25OXFxcoJfvnhQm6zZ1HZSk+QRj\nxGyInQwzb/CNHToDLv9T04qzAHGTYNYtsPtt/T7lS8Gft2EYPYJABaPeOefQGMT/OuceB2KOcc5G\nYKKIxItIBHADsCyQm4mIAM8Be5xzfwpwjj0fr2Ac/ATqq33xibAIuHtDU8E4Guf/VNddDJqoAmIY\nhhEAga70LhORH6PptGeLSAgaxzgizrl6EbkbWAWEAkudc7tE5C7P8adEZCiQAPQFGkXkPjSj6lTP\nvXaIyFbPJX/inFvRxt+vZ1GQpNtGT45A8/hEoMQMgRtegrDe7TMvwzBOCgIVjOuBm9D1GNkiMhr4\n/bFO8jzgVzTb95Tf52zUVdWczzhG2u5JSWGSxiSKUvS7fwZUWxl33onPxzCMk4qAXFKeB/tLQD8R\nuRyods4dK4ZhtDeFydpTu68n2az/qKOPNwzDaEcCLQ1yHbABuBa4DlgvIl8J5sSMZlQVa3mPQeNh\n7FkQM8yXEWUYhtEBBOqS+im6BiMXQETigA+AN4I1MaMZhZ74xcDxcNpXoaKHpxAbhtHlCFQwQrxi\n4aEAq3TbMZRmwQtXwqi5+n3gOOgTqz+GYRgdSKCC8Z6IrAJe9ny/nmbBbCNIJP0H8vfpD8DA+M6d\nj2EYJy0BCYZz7gERuQZY6Nn1jHPuX8GblnGY1M8hsp/mjEX2hXBLhTUMo3MIuOOec+5NdOW10ZGk\nfq5B7nPuh+rizp6NYRgnMUcVDBEpA1xrhwDnnLM+nsGkLFsbJc29HUac3tmzMQzjJOeoguGcO1b5\nDyOYpH6u2zFndu48DMMwsEynrs3BjyG8Dwyd2dkzMQzDCDyGYXQQtRWQtxd2/xs2/Z92zwu1fybD\nMDofexJ1Fs4TGpJmJbNeuhZS1+rn02+DS49ZssswDKNDMMHoDJyDJxfCxAu10ZGXqiI4tA5m3Qzz\nvwODp7UUFMMwjE7CYhidQeZmyN0Fm1+A+loVEOe0z4Vr1AZHQ6abWBiG0aUwC6Mz2O3pI1VVBHvf\ngXWPw+Cp2h0vIkY76BmGYXQxTDA6GudgzzKIPwdydsO/vwt1lZCRoF3w4s+F0KP2pjIMw+gUzCXV\n0eTs1L4WM66BU76iYnH6rVpUsLYcxp/f2TM0DMNoFbMwOprtr4KEwuTLYNz5gMCihyE9Ad7+Dkxa\n3NkzNAzDaBUTjI6krgq2vAhTL4foON235BHdjjsXfrCr8+ZmGIZxDILqkhKRxSKyT0QSReShVo5P\nEZF1IlIjIve35dxugWtWhmvX2xronnN758zHMAzjBAiaYIhIKPA4sASYBtwoItOaDSsE7gX+cBzn\ndh3qqmHlQ1B8yLdv3RPw6ClQ4OmUV18L65+EQRM14G0YhtHNCKaFMQ9IdM4lO+dqgVeAK/0HOOdy\nnXMbgbq2ntul2PG6isH2V337Ej+AkjT4+5dg+2vw6lchaxuc84CtrzAMo1sSTMEYAaT5fU/37GvX\nc0XkThFJEJGEvLxO6HPtHGx4Wj+nb/Lty9oKo8+Ehlp46w448D5c/ijMvL7j52gYhtEOdPugt3Pu\nGeAZgDlz5rTWuyO4pG2A7B3Qq5+upXAOStKhsgBmXK0ps4XJEBIGsRM7fHqGYRjtRTAtjAxglN/3\nkZ59wT63Y1n3V22hes4DUJEHxalqXQAMOw3CInUVt4mFYRjdnGAKxkZgoojEi0gEcAOwrAPO7TjS\nNsCe5bDgO75AdnoCZG7VtRZDZ3Tu/AzDMNqRoLmknHP1InI3sAoIBZY653aJyF2e40+JyFAgAegL\nNIrIfcA051xpa+cGa67HhXOw6qcQPRTOvAdCIyGstwpGQSLETYHw3p09S8MwjHYjqDEM59wKYEWz\nfU/5fc5G3U0BnRsM6hsaWZdcQGx0JFOHtaFFecYmSN8Al/8ZIvrovuGzYP9KqCqGKZcFZ8KGYRid\nxElfS8oBd/1jEy+tT23biYfW6XaynzDMvAEq8qG6GMYsbLc5GoZhdAW6fZbUiRIeGsL8cYP47EB+\ny4M15Vo5NixSvzc2aKXZKV+CtPUwYCzEDPGNn32b9rKoyIXoIS2vZxiG0Y056S0MgLMmxpJSUEla\nYWXTA/93Gbz7A9/3Pcvg9a/B9lc04D3qjJYXCwmBmKG2OM8wjB6HCQZw1oRYANYm+lkZZTmaHpv0\nkW/f/lW6/eT3UJ4Do+Z13CQNwzA6GRMMYMLgaIb0jeRTf8FI/Uy3pelQmgmNjbpaOzwKilL02EgT\nDMMwTh5MMAARYeGEWD5PzKex0bNYPGWtb0D6RsjcApX5sOjnEBKu3fEGd916iIZhGO2NCYaHcyfF\nUVRZx7b0Yt2RulYX44VGarziwCqQEDj1epjzDe2YF3rS5wwYhnESYU88D+dOiiM0RFizK5NZfcsg\nb6+KQ30NJK2B6hJ1QUUNhEv/X2dP1zAMo8MxwfDQPyqCuaP7cu3Ga2G9p2zV2LO0iOC6/4XQCLju\nhc6dpGEYRidiLik/bhuSzKjGDMqn3wSLH4ERc1Q0AC77I4yc3bkTNAzD6ETMwgD4/K8w4SLOKVtJ\ngYthxbDvc8v8SXps0mK4dwsMHNe5czQMw+hkzMKoLIS1j8FzF9EnZTUfRCxiTWKJ77iIiYVhGAYm\nGBrEvuND6D8GXCOZ465lfXIBdQ2NnT0zwzCMLoUJBkD/UXD7avj250yaPpuK2gZ2ZJQc+zzDMIyT\nCBMMLxFRMHgq88cNBGBdUkEnT8gwDKNrYYLRjEHRkUwZGsPnSa1UrzUMwziJMcFohTPHx5KQUkR1\nXUNnT8UwDKPLYILRChdOHUxNfSNvbc7o7KkYhmF0GUwwWmHB+EHMGt2fx9ckUltv2VKGYRgQZMEQ\nkcUisk9EEkXkoVaOi4g85jm+XURO9zv2fRHZJSI7ReRlEekVzLk2mxf3XTiJjOIqXt+U1lG3NQzD\n6NIETTBEJBR4HFgCTANuFJHm9cCXABM9P3cCT3rOHQHcC8xxzs0AQoEbgjXX1jhnYiwzRvTllQ0m\nGIZhGBBcC2MekOicS3bO1QKvAFc2G3Ml8IJTvgD6i8gwz7EwoLeIhAFRQGYQ59oCEeHyU4ezI6Ok\nZetWwzCMk5BgCsYIwP/1PN2z75hjnHMZwB+AQ0AWUOKcW93aTUTkThFJEJGEvLy8dps8wJIZQwFY\ntSu7Xa9rGIbRHemSQW8RGYBaH/HAcKCPiNzc2ljn3DPOuTnOuTlxcXHtOo8xg/owdVhf3ttpgmEY\nhhFMwcgARvl9H+nZF8iYC4GDzrk851wd8BZwZhDnekQWTx9KQmoRZ/7uP/z5/f2dMQXDMIwuQTAF\nYyMwUUTiRSQCDVovazZmGXCrJ1tqPup6ykJdUfNFJEpEBFgE7AniXI/IDfNGcdWsEYSGCu/uyOqM\nKRiGYXQJgiYYzrl64G5gFfqwf805t0tE7hKRuzzDVgDJQCLwLPAdz7nrgTeAzcAOzzyfCdZcj8aQ\nvr348/WnccXM4aTkV9i6DMMwTlqC2kDJObcCFQX/fU/5fXbAd49w7i+AXwRzfm1h4uAY6hsdqQUV\nTBwS09nTMQzD6HC6ZNC7KzJhcDQA+3PKO3kmhmEYnYMJRoCMj4tGBA7klnX2VAzDMDoFE4wA6R0R\nyqgBURzINQvDMIyTExOMNjBxcDSJOeXsySplV6Z15DMM4+TCBKMNTBgSTXJ+Odc9tY57/rmls6dj\nGIbRoQQ1S6qnMXFwDHUNjrqGespq6sksrmJ4/96dPS3DMIwOwSyMNjB/3EBOGdGPR64+BYC1idbG\n1TCMkwcTjDYwckAUy+85i+vnjiI2OsIEwzCMkwoTjONARFgwPpa1SQX86f39/Gn1vs6ekmEYRtAx\nwThOFo4fRF5ZDY/95wB/XZNoPTMMw+jxmGAcJxdOG8K8+IH85NIpALyWYJ35DMPo2ZhgHCex0ZG8\n9q0F3HnOeM6bFMerG9Oob7DChIZh9FxMMNqBG+eNJreshisfX8vzaw929nQMwzCCgglGO7Bo6hB+\ntHgKISL8avluNqUWdfaUDMMw2h0TjHYgNET49nnjefVb8xkcE8mv39mNVm43DMPoOZhgtCNREWHc\nf8lktqYVc8mjn/Drd3bT2GjCYRhGz8BKg7Qz15w+kpySatYlF/DcZweZNCSa6+eO7uxpGYZhnDBm\nYbQzoSHCPYsm8uLtZzBv7EB+u2Iv+eU1nT0twzCMEyaogiEii0Vkn4gkishDrRwXEXnMc3y7iJzu\nd6y/iLwhIntFZI+ILAjmXNubkBDht1fPoLK2noff3mkxDcMwuj1BEwwRCQUeB5YA04AbRWRas2FL\ngImenzuBJ/2O/QV4zzk3BZgJ7AnWXIPFhMEx3H/xZFbuzD68sO/DvTks+cunFFXUHh635VARB/Mr\nOmuahmEYARFMC2MekOicS3bO1QKvAFc2G3Ml8IJTvgD6i8gwEekHnAM8B+Ccq3XOFQdxrkHjjrPH\nsXDCIH61fDeFFbUs/SyFPVmlPPbhAQBySqu56dn1PPjGtk6eqWEYzUkrrKSksg6AX7+zm6Wfdew6\nq4ZG16W8E8EUjBGAf72MdM++QMbEA3nA8yKyRUT+JiJ9gjjXoBESIvziS9OprG3gD6v3sTYpn769\nwnjxi1RS8iv44+p9VNU1sDGliOyS6hbnO+f4eH+erSI3jA6mtr6RLz++lt+v3gvAsm2ZHV4C6K4X\nN/HD17rOy2RXDXqHAacDTzrnZgEVQIsYCICI3CkiCSKSkJeX15FzDJhJQ2I4b3Ic/1x/COfgb7fN\nJTw0hIsf/YTXN6Vz8bQhAKzcmdXi3H9tyeC2pRtYuTO7o6dtGCc1nyXmUVBRS2pBJXUNjeSX17A/\np4zK2voOuX9NfQOf7M9jR4avHfTuzFKue3odibnlh/f9cfU+rnx8bYek8AdTMDKAUX7fR3r2BTIm\nHUh3zq337H8DFZAWOOeecc7Ncc7NiYuLa5eJB4M7zh4HwKzR/ZkXP5DX71rAzWeMYdGUIfz+2plM\nGRrDawnp3LZ0AzN/tZp5v/mAlTuy+PMH+wHYltYtPXJGN+VgfgVbDp3cFQve2aYvcFkl1eSV1eAc\nNDrYlVnaIfffmVFKTX0jWR7PQ1phJbc9v4ENBwtZti3z8LhNqUU45wgJkaDPKZiCsRGYKCLxIhIB\n3AAsazZmGXCrJ1tqPlDinMtyzmUDaSIy2TNuEbA7iHMNOmeOH8TXF47l+xdOAmD68H78/EvT+Ntt\nc+jXO5zLThnGnqxSElIKufzUYcRGR/LtlzaTVlhFTGRYk7cMwwiU93fnMPc3H1BaXdem8/5r+S7u\ne3VrkGZ1fGSXVFNd19Du121odNzxQgJr9uUe3ldd18Dq3TmH75td6nMXH+nlzTnH/6092G715BJS\nCgEor6mntLqOX7+zm+q6BsYMimJdkjZva2x0bE8vYebI/u1yz2MRtIV7zrl6EbkbWAWEAkudc7tE\n5C7P8aeAFcClQCJQCXzd7xL3AC95xCa52bFuh4jGMo7EzfPHUFHbwFfPGM2ogVGUVdfxnZc2ExkW\nwtB+vXh7SyaNjU3fIuoaGgkP9Wn+8m2ZvLLxEC984wxCW3nb2JddRkyvMOtD3g14fu1BoiJCT3jR\nZ0JKIXllNWxILuRCj+szEHZmllJSWYdzDpFjv7km5ZXz1uZ0fnjR5HZ/021odDy+JpG//OcAt58V\nz08unRrQeVvTiqmqbWDB+EFN9r+1OZ2E1CJ+e5W2Wk5IKeT93TmEinD+5MEAfLw/j/Kaes6eGMun\nB/IPu4BCRK/b2hwfeGMbb23OYFCfCL6+MD6gOe7LLuOJjxKZNao/V50+kn69ww8fS/CrSZdVXM2+\nnDLOmRTH6IFRPPtJMpW19WQWV1FeU8/MUR0jGEGNYTjnVjjnJjnnxjvnfuPZ95RHLPBkR33Xc/wU\n51yC37lbPa6mU51zX3bO9Wj7eECfCB5aMoVRA6MAiOkVzj9uP4Nnb53DKSP6UV5TT0qBpt4++0ky\nUx9+j0k/W8nP/72T+oZGiipqefjfO1mbWMC+7LIW129odNz83HoeOEmzsSpqOsbv3B7UNTTyp9X7\nefbT1t9UGxsdSz87SHFlbavH/Ukt0MZe6w8WBHz/3DJ1wdQ2NFJYcex7OOf48Zs7eHxNEgf8fOv+\n1Dc0HreP/fm1B/nT+/uJCA3hk/2Bxyl/9vYO7npxE1W1PqukvKaeX7+zm3+uP3TYUvDGB9clF9Dg\nmeMn+/OIjgzjqlmap+MViTljB7I9vaW1v2pXNm9tzmB8XB8KKmoD/u/t8TWJ/HtrJr9cvpt7X95y\neL9zjoSUQsbFaq5PakEFaYWVjIvtw5njB1Hf6NiYUsTWNJ3LaaP6Bfx3ORG6atDb8CAizBih/zHs\nyCihuq6BJz9OYsLgaK6eNZIX1qVy47Nf8L1Xt1JSpW6HTalqyjrn2Jutb4qbDxWRV1bDF8mFh9ME\nTxYyiqs4/dfvs3pX+yYO/G7FHn7yrx3tek2AjSmFlNXUk5xX3qoLZmdmCf/1zm7e3tI8JNiSQ55O\nkBsOFgZ8/91+Pnp/V0xzkvPKefSD/bywLpUNHvfJzlZcp845bvrber77z80Bz8GfD/fmMmVoDN8+\nbzz7csoCEsqSqjp2ZZZSUlXHcj9//98/T6Goso6IsBD+8UUqjY2OVbuy6RMRSklVHXuy9Hdfl1zA\n3LEDGDlAX+C2pRUTHqoWyKHCyhZC+tL6VEb07829iyYCkFakf/fa+kb2ZZdR10qWY0lVHat2ZXPr\ngjHcf/EkPt6fd1iY9ueUU1RZx5dmDgf036/RwdhBfZgzZiDhocLnSflsSysmOjKMcbHRbf2zHhcm\nGN2ASUNiiAgLYWdGCSt3ZlFYUcuPFk/hj9fN5HdXn0JmcTWf7M/jGwvjiYuJJCG1iAM5ZVz62Gcs\nfvRTvv3SJt7zvEU1NLomvtrm7M8p44vkwN9G20JibnmT7I4j4Zzj3e1Z7VZSZc3eXGrqG/moDW+n\nx8I5xxub0lm+LbPd8+TX7NV/n0an/x7N8QZdUwqO3hbYOcehwkpCQ4SdmaWUB/jWuzvLJxg5RxGM\nJz5K4tEPDvCLZbuYMDiaXuEhTQLCL6xL4bWNaby/O4cNBwtZtSub3LIjX681qusaSEgtYuGEWM6I\nH4hzsDHlyM6GB9/Yxk/+tYOElEKcg6iIUP6+LgXnHFW1DTz7aTLnT47j2tkjWbYtk+XbM8kqqeYe\nz4N+bWI+OaXVJOdVcOb4WIb16wXA3uwyBsf0YuEEdW/5i3VKfgVrEwu4Ye4oRns8BGmFVbyy4RCn\n/moVlzz6Ca9sbJmOu3xbJjX1jVw7exRfWxhP/6hw/vz+fr5ILuCOFxLoFR7CV2aPJDREWJuk/0+O\njS7JULEAABrsSURBVO1D74hQZo0ewIodWXyelM+pI/t1SMAbTDC6BeGhIUwdGsMHe3J5+uNk4j1m\nKWjzps9+dD7rfnwBP7l0KnPGDCAhpYj/eW8fmcVVfPm04XyeVMBL61M5b3IccTGRvL87h4/25bYa\nvPvNu3v45t8TmpjxJ0pDo+Nvnyaz5C+fcNOzXxwxcJmUV05lbT2vJaTx3X9u5ncr9rb5XrX1Ld/k\nvG6MTUd40GSXVLe5h0lSXgUFFbWUVdcfdvscifzyGr72/IZW3SmvbDjU4u1/zb48xgzSB8+erFIK\nK2qbPGh3ZepbfKrHRfnW5nQKWhHXoso6ymvqOW9SHA2N7nAQ9UjkllWTUVzF7sxSoiJCAcguaV20\nGxoda/bmcv7kOO6/eBKPXn8aU4f1ZadnbvUNjTyyci8PvrmdB9/czuCYSBodLN/WMnXcn8ZGxx9X\n7ztc+WDzoSJq6xs5c/wgZo7qT0RYCOuP8EJTXdfA21szeWXDId7emklEaAg/uGgSuzJL2ZpWzMaU\nQoor67jtzLHcumAstfWNfO+VrYSHCjfOHc3EwdGsTSpgnefhvGD8IAb3jTz8+w7t14tTR/bnjPiB\nPPNJ8uH/1l7ecIjQEOG6uaMOu5TTCitZvj2T2OhI+vUOZ6fHjfV6Qhpb04qprK3nxS9SmTI0hhkj\n+hIdGcY3Fsbz8f48bnjmCypq6nn5jvmMGhjFkJjIw5ZPvMdF9f0LJ1FQXktSXkWHxS/ABKPb8I2z\n4skvr2Fvdhm3zB/T5I1CRBjWrzehIcLsMQPIKK7igz053HbmWH5/7UzGxfWhuq6RJTOGcuHUwazc\nmcXXnt/I3S9vbuFX3p2lb6Krd7fNffPO9ky2p7cUoC+SC7jk0U/473f3MH14P3LLanhzc3qLcakF\nFVz850847/cf8YtluwgNEd7bmdVEuBobHd/952YeWbm3VX/4v7dmcOqvVvHvrb63v7qGRj5PKiAi\nNIT9uWXkl9dw/+vbeOrjJDalFvK7FXs47w9ruPapz9v09rvR7+HrfUi2Rll1HV97fgMf7cvj75+n\n6PiMEoora8kvr+Gnb+/knpc3H/Z5784sJTG3nFvmj6FPRCh7ssr45t83cucLmw5f0+sySi2sJLWg\ngh+8to0nPkpqcW+voFw5awRhIcI6z4P24bd3tsjkcc5x63MbuPQvn7LhYCELxg1C5Mguqa1pxRRU\n1PLlWSO4+4KJzBjRjxnD+7E7s5TGRseerDIqaxsYPTCK4so6Hr58GqeM6HdMN9q29GL++mEiL36R\nCsDniQWEhgjz4gfSKzyU00b1P+z+8s7775+n8NG+XDamFFJb3+gRpkxOG9Wf6+aOIjRE+GCPWjmh\nIcLcsQOZPDSGV+6cz5+vn8lL35xPv6hwzhw/iPXJBbz4RSp9e4UxdVhfIsNCiY2OAGBoX7U2vnP+\nBLJLq/nXlnRq6ht4fVM6F04dzJC+vRjUJ4KoiFDSiirZlVnKwvGxTB0Ww/7cMvLKanjgje1c8+Tn\nLH70U/bllPGd8yccTir41rnjeOrm2Tx9y2zeu+8cZo0eAMAwT5JKv97hDIjSoPiC8YN4+Y75zBrd\nn8tOGXbUv2l7YoLRTbjytBFs+tlFLLt7IbedOfaI4+aMHQhARGgIt8wfQ3hoCL++cgZThsZw8bSh\nXDVrJOGhIZw/OY60wirWetLzQN+E88r0jfKNTfpQd87x2sY0cj0PjtYe1FklVdz78hZuXbqBjOKq\nJsd+8tYOqmob+N+bZvGv75zJaaP689THSS1Wrr+1OYNG5xjWvzcDoiJ49PrTqKhtaCJcb2/N4N3t\nWTz1cRL3vLyF6roGGhsd29KKWbEjiwff2E59g+PBN7Yftp62/P/2zjw+yurc499nskxIMtn3hSwk\nhBASwhoEWQVZFURbd6VWq3Wr16VqtVdva7Wtt+3HBStq6bWi0Ctg4Qq4gBTBCiHsOwlhSUhMIBuE\nBBKSc/943xkmkMAEy0yU8/188sk7Z87MPPOcM+d3znOW91At9adOc9PgRJSC5xbtYP6GUn67bDfX\n//lr3lpdzKDkMFoVrN57xhcrd1dSVHluOMhO/v5qwgN88fGS8y55nrlyH7vKj9M3MYR/7auivK6R\n6W/8i5/P38rSbeW0tCoqjp3ixaW7eHHpLqa98RU2qzcTs2PpFRvE0m3lbDxUy7bDdTQ2tdBiNsZg\n9GK3mD3Xz3dWnBMas89fZMbYuKJHOEu2llNS3cB7aw+ycGPbhnt14VF2f3Oc4yebqTx+ipyEECIC\nrVS0c/oAGGeieVmEUT2jHGl94oOoP3WaQ9UNDkF9/+48/vHAMKbkxDKtXzzbDtexYldFh/760iwD\n+yS9PeRi8zMayryUMLYfrqPy2EmUUry0bDfPLd7BE/O3smJXJT5ewhWpxug7LzWMID8fBnQP5Z97\njrBufxV94oMJsBqLQ4ekhnNdvwQGpxi/mbuHp5IQ2o2CgzXkpYY7VhrGmGGpaFMwRqRHkB0fzCvL\nC/lo42GqTzRxa14SYHTeEkP9yd9vjGay4oPoGW2jqKLe0aHKSTAWsfx1xiCuNecoAKzeXkzoE8P4\nrBgibVZHuj0slhwR0GbFWt/EED66f5hjjtMdaMH4DuHrbSEnIaTdJbN2suKCCPLzZnr/eEelG5YW\nwSePjCA0wJfBKWHs+tUE/nzbAEL8fZiXfya2ah/2Dk4JY03RUcpqG9lTcZyfL9jKgx9sYm/FcYa8\ntIK3vyxu85nz8ktQGOGghz7Y6FhpUtvQRPHRE9yS150pOXGICA+MTqOkupHZTj1cpRQLN5UyrEcE\nix4YxponxzA5O5b4kG6Ohq2xqYWXP91Ddnwwv5jUiyXbyrlzdj53zM5n6syvuP/9jUQEWvnkkeFE\nBFodk9Gr9lbiZRHuH5WGRWDJtnKy4oJY9rPhzLylPxueHce7PxpMRKDVMcdR19jMve9t4EUzJKaU\n4nBtI+sPVDsa5fz91QxJDScjxsb2w3W88PFO7n7XWORXUt3gCJt8XVzFgKRQHh6TRmNzC08t2EZT\nSyuf7azgrS+LyYi2Mb1/PO+vO8Q7q4uZnB3L8sdGEh/SjcxYG5WmgLe0KrYdruNA1Qkam1sYmBRK\nc4vic3OvwKHqBvacNd9xyAyVJYb5M71/PKU1jTz7j+0A50zEvrNmP5E2K2/cOgBvs0cfE+TX7gij\n6XQrn+2oYFByKMH+Z5aBZsUZDdf2sjoKDlYTH9KNxDB/chNDEBFuGJBArxgb9/ytgPfMEQS07YSs\n2mvM3+woO0bxkXq2lNYxrEeE4/nr+xsx/Zc/3cPMlUW89WUxQ3uEc+T4KeasPUj/7qE8MDoNgJE9\njY28IzMi2VF2jE2HaskzxaE9EsP8WfTglfx0VA/uG9nDkR4TZPTwo83wlLFEvjdldSf55aLtdA/z\n58q0CKf36eaYy8mKCyI9KpDjp07z+c4KRGDOj/MoeGYsozKicIV4c4SRYoYpPYm+gdL3DB8vC8se\nGUF4gG+HeSwWwc/ixfR+Cby39gB//Hwvk7JjHILx7ORMrn39KxZtLsPHyxCn/APVTH39KxqbW/jj\n53uZnBPL1tJagvx8mLf+ECN7RjI5O5Yn5m9ldeERRmVEOVZ89Ot+JsY6NjOK8VnR/P6TPQxOCSc3\nMYSCgzWUVDc6NjXaBXF6/3hmrixi35F6lm0rp7zuJH+6MZchqeFE2fx4Yv4WvC0WnrumNz2jbfSJ\nCybY34c7hybx4tLdlNY08OmOCgYmhRIT7EdmbBA7yo5x78geZMYGkRkb5LBrZM9IVuyuoMVcNdPU\n0sra4iozzr3JsfTyl1N6MyI9gsO1jfxkRCpB3bxZuPEw/9pXhVKwtbSW5xfvYGf5Mb56cgw7Dtdx\n78hUrugRjq+XhVV7j5AVF8ShqgZKaxp5YnwGt+UlkZsYwtjM6DZ7ZOz2jekVxRe7K9lcUkNMsPH8\nxOxYCg7WsGJXBXHBfpQfO8nizWWsD6mhtLqBqCA/9ledIDrIip+PF+OzYgjw3c6qvUfw9bLQ1NJK\nUWU9gVZv3lldzJd7j/D41T2Z0CeG7f81Hj8fL6KD/Cg1V/u0tireW3uQqvpTLN9VSWFlPfeMyGlT\nr3pG2/DxEr7eV0XBgZpz9j8Ed/NhwU+H8tDcTTy3aDtpkYG8s7qYo/Wn+PC+oTQ2tbC5pJbByWHk\nH6jmqQXbaGlVTOt35gi65IgA7hqWwiyz03Jdv3j+8IO+THltDTvLjzE8PYIr0yPIf+Yqomx+jrJ9\n+dM9nG5VDE7uWDAAAq3ePDmhV5s0ew/fPtIAYyR/48BE/l5Qwk2DE9uEiO0rq0SgV0wQzS2GIC7Z\nWk5aZKBjhOMq9s9PcdNKqPOhBeN7SLyLG/NmDE1m1d5KXvuikA/WHXT0KnMSQshNDGHJtjLCAqyk\nRgaQHhXIqr1HeO3mfjz2v1uY9Opqap2W574wLYkRPSP49cc7WbS5jFEZUWw6VIsI5DjtQhURfn99\nXya9uprb/7KOmwYl8umOCvx9jUbNmTuHJjN7zX6eX7yDjQdrGNc7miFmuGFav3gyYmwEWr0dE412\nxmZG8+LS3cxcWURRZT0zpvUBYHxWDErBpD5tPwdgVEYkCzaWsrmklv/bUoZFoKGphQ83lLBs+zf8\ncGACVfVNvLR0F29088Hm582YXlF4ewlz80sI9ffhZHMrTy7Y5hDe174o4nSrYmBSGP6+3uSlhrG6\n8Ch3DUvhYHUDM1cWcW3fOIL9fbjjiuRzbLoiNZwom5XHr85gb8VxNpfUkniiCR8vYVxmNL/+eCcN\nTS1MyYll35ETjnkMuyD4eAn9Eo04uL+vEeaav6GUO4cm8fbq/ewoO8asVfs4WN3AtNw4R6jTz8eY\n8I4JtlJgLtF+ZUUhr6woRAQiA63Mun3AOeXl623h2r7GaAnOhEedCbB688pNuUx8ZTW3vrMW++Di\nnTXFJIUF0KrgkbHpzPjrevIPVDMkNYy0qLYN5YNj0li8pYykcH9+e302Fotw/+gePDR3E6N7Gb12\nu1iA0cuPtFk5Wn+KQRcQjPY4OyRl5xeTMgkN8HWEo+zY62NKRAABVm/STfuPnzrNuITOh4/scxjJ\nEXqEofEg3cP9WfHYKL7eV8XNb69l2fZvHMP4KTmxvLBkFz5ewq15STw7OZOahmYibVYKK+uZtWof\nv56ahdXHi31H6hmdEYm3l4XJObEs2lxGQ9NpNpfUkhFtNOrOBPv7MOfuPF74eCdvr95Prxgb79wx\n8JyeV0SglR8PT+XVFYV4W4SnJ7bt+TmPEJxJjQwkNTKAufkleFmEiaZAPHxVOg+NSWt35/Lw9Ags\nYuyt2HiohtuHJDFn3SF+u3Q3Xhbh8fEZ+Pl4Me31r2hubWXencYKlgFJRoP86NUZ7CyrY25+CZE2\nK6dbWpmz9iAi0N+cvLxhQAIVx04yKTsWX28L03LjzhG7s79H/jNjAchNDGHd/mry99fQr3soCaHd\nsHpbOHW6lez4YEZlRGH1tvDg6DSu6BHO0wu3MW99SZv3v2d4KnWNzTw4Jp05aw+xYEMphZX1vDCt\nD7cNSTrn82OC/KhtMPYKvPpFIdf3T+C/f5CDUnS4jPOl6dnUNDSxck8lQzoI/9j8fPjTjbnc/W4B\nj13dkzWFR3lleSE+XhbCzbCpfXL7lrxz7bL5+bD80ZF08/Fy2DElJ45ByWHnNOpgdFKu6xfP7m+O\ntwmhuUp6VCDeFiE5vO2B2cH+Pjx1Vp0ESAw1Gnh7iC480Ep4gC9VJ5rIuYj5hoFJoYzpFcVQp9Cc\np9CCoWFIahh9E0PYUlLraIQnZhuC0dyiGJ4egbeXxTEn8h9j07l/VA9HT9SZqbnxzM0v4bMdFWwp\nrWVC1rm9eTB6X3+ZMYjK4ycJD7B2OC9zz/AUFm4s5dq+caRGuj4kH5cZzawjxVyZFkF44JkJxI6O\nuQjx9+U312XzmyW7aFVwS14S28uOseFgDaMzIh091iUPD8diMSYowQg5/PPxUSSF+7Oz/Bjz1pdw\n74hU9nxznA83lNIrxuZopKbmxjM190x4pTPfJzcxhI+3luNtEZ67ZhAWi5AU7s/einqy4oPp3z2U\nSU6rZX41tQ9eFmGC02gqI8bG23cMNOyOtfF1cVUbQT0be+P7+IdbSI8K5IVpfRARzndSiK+3hTdv\nG8D+oydIj7Z1mG9QchibfjkOi0W4KjOaGbPzyU4I5t4RPfD2sjAxO4aj9acYn9X+cSbthXXaEws7\nrh4n0h7jekez5skxbUJS58Mu0llxZzo06dGBVBVXk30RZz6FB1qZPWNQp193KdCCoUFE+OnIVO6b\ns5FsswcUH9KN/t1D2FpaR15q+Dn52xMLgMHJYSSGdeOphVs52dxK7gXWiDuHDtrD5ufDysdHtTkz\nyxWuzoph1pfFjqMdXOHmwd25KjOKoop6MmJsXJkWwYaDNdww4MyByt18z/3eyeba+Ky4YL54bBRJ\nYf6s3FPJhxtKGZgc2im7O8IeSrlvZA9HzzUpPICiynoyY84dafl6W/iNeVZSe/SODWLToVqG9ghv\nI6jO2BvI4ydP8/Ydfdr97u3h620hI6ZjsbBjHx3Eh3Tj80dHtnnuR8NSXD6P6VIjIi6LBUBGtI0n\nxmdww4CENmkFB2ro3cGo+LuCFgwNYMT3594zxLHEEIxeWaE5MeoqFovw3l15/O6T3Xyxu/LfMozu\nrFgADEgKZenDw8mMvXDD5UyUzc8hYrfkdae5pZVxnTi0z76xalhaBGMzo7muX8IFXuEafRNDmH/f\nFY61+QDT+8WTFObvckPujF10rsmJ6zBPrDnBPiUn1jF3pLkwFos4VmrZuX90GuOzYi6qrLoS0pVu\n//dtGThwoCooKLhwRo1bcPWkU437qTnRxOsri3js6p74+7bfIVBKMWfdISZnxxJ2nlV3mu82IrJB\nKTXQpbxaMDQajebypTOCoTfuaTQajcYltGBoNBqNxiW0YGg0Go3GJbRgaDQajcYltGBoNBqNxiW0\nYGg0Go3GJbRgaDQajcYltGBoNBqNxiW+Vxv3ROQIcPCCGdsnAjh6wVzuR9vVebqqbdquzqHt6jwX\nY1uSUirSlYzfK8H4NohIgau7Hd2JtqvzdFXbtF2dQ9vVeS61bTokpdFoNBqX0IKh0Wg0GpfQgnGG\ntzxtQAdouzpPV7VN29U5tF2d55LapucwNBqNRuMSeoSh0Wg0GpfQgqHRaDQal7jsBUNEJojIHhEp\nEpGnPGhHooisFJGdIrJDRH5mpj8vIodFZLP5N8lD9h0QkW2mDQVmWpiIfC4iheb/f88NrF23KcPJ\nL5tF5JiIPOIJn4nIbBGpFJHtTmkd+kdEnjbr3B4RGe8B214Wkd0islVEPhKREDM9WUQanXz3ppvt\n6rDs3OWzDuz6u5NNB0Rks5nuTn911Ea4r54ppS7bP8AL2AekAr7AFqC3h2yJBfqb1zZgL9AbeB54\nvAv46gAQcVba74GnzOungN95uCy/AZI84TNgBNAf2H4h/5jlugWwAilmHfRys21XA97m9e+cbEt2\nzucBn7Vbdu70WXt2nfX8H4D/9IC/Omoj3FbPLvcRxmCgSClVrJRqAuYBUz1hiFKqXCm10bw+DuwC\n4j1hSyeYCrxrXr8LTPOgLVcB+5RSF7vT/1uhlPoSqD4ruSP/TAXmKaVOKaX2A0UYddFttimlPlNK\nnTYfrgUSLtXnd8au8+A2n53PLjFuUv9DYO6l+OzzcZ42wm317HIXjHigxOlxKV2gkRaRZKAfsM5M\nesgMHcx2d9jHCQUsF5ENIvITMy1aKVVuXn8DRHvGNABuou2PuCv4rCP/dLV6dxewzOlxihleWSUi\nwz1gT3tl11V8NhyoUEoVOqW53V9ntRFuq2eXu2B0OUQkEFgAPKKUOgb8GSNklguUYwyHPcGVSqlc\nYCLwgIiMcH5SGWNgj6zRFhFf4FrgQzOpq/jMgSf9cz5E5BngNPC+mVQOdDfL+lHgAxEJcqNJXa7s\nzuJm2nZM3O6vdtoIB5e6nl3ugnEYSHR6nGCmeQQR8cGoCO8rpRYCKKUqlFItSqlW4G0uYejifCil\nDpv/K4GPTDsqRCTWtD0WqPSEbRgitlEpVWHa2CV8Rsf+6RL1TkRmAFOAW82GBjN8UWVeb8CIe/d0\nl03nKTuP+0xEvIHpwN/tae72V3ttBG6sZ5e7YKwH0kUkxeyl3gQs9oQhZmz0L8AupdQfndJjnbJd\nB2w/+7VusC1ARGz2a4wJ0+0YvrrTzHYnsMjdtpm06fV1BZ+ZdOSfxcBNImIVkRQgHch3p2EiMgH4\nOXCtUqrBKT1SRLzM61TTtmI32tVR2XncZ8BYYLdSqtSe4E5/ddRG4M565o7Z/a78B0zCWG2wD3jG\ng3ZciTGU3ApsNv8mAe8B28z0xUCsB2xLxVhtsQXYYfcTEA6sAAqB5UCYB2wLAKqAYKc0t/sMQ7DK\ngWaMWPGPz+cf4Bmzzu0BJnrAtiKM+La9rr1p5r3eLOPNwEbgGjfb1WHZuctn7dllpv8PcN9Zed3p\nr47aCLfVM300iEaj0Whc4nIPSWk0Go3GRbRgaDQajcYltGBoNBqNxiW0YGg0Go3GJbRgaDQajcYl\ntGBoNF0AERklIh972g6N5nxowdBoNBqNS2jB0Gg6gYjcJiL55mFzs0TES0TqReRP5j0KVohIpJk3\nV0TWypl7ToSa6WkislxEtojIRhHpYb59oIjMF+M+Fe+bO3s1mi6DFgyNxkVEJBO4ERimjMPmWoBb\nMXabFyilsoBVwHPmS/4GPKmUysHYvWxPfx+YqZTqCwzF2FUMxumjj2DcxyAVGHbJv5RG0wm8PW2A\nRvMd4ipgALDe7Px3wzjorZUzB9LNARaKSDAQopRaZaa/C3xonskVr5T6CEApdRLAfL98ZZ5TZN7R\nLRlYc+m/lkbjGlowNBrXEeBdpdTTbRJFfnlWvos9b+eU03UL+vep6WLokJRG4zorgBtEJAoc91JO\nwvgd3WDmuQVYo5SqA2qcbqhzO7BKGXdKKxWRaeZ7WEXE363fQqO5SHQPRqNxEaXUThF5FvhMRCwY\np5k+AJwABpvPVWLMc4Bx1PSbpiAUAz8y028HZonIr8z3+IEbv4ZGc9Ho02o1mm+JiNQrpQI9bYdG\nc6nRISmNRqPRuIQeYWg0Go3GJfQIQ6PRaDQuoQVDo9FoNC6hBUOj0Wg0LqEFQ6PRaDQuoQVDo9Fo\nNC7x/1ZpjUI+GyHZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cb80624160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSProp быстрее SGD, поэтому чтобы достичь верности:\n",
    "* на тренировочной выборке\n",
    "* на контрольной выборке\n",
    "* на тестовой выборке\n",
    "\n",
    "Понадобилось всего 10 итераций. Выше приведен график изменения верности и потери при росте числа периодов.\n",
    "\n",
    "Теперь попробуем еще один оптимизатор: Adam.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OPTIMIZER = RMSprop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=OPTIMIZER,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=BATCH_SIZE, epochs=200,\n",
    "                    verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"\\nTest score: {}\\nTest accuracy: {}\".format(score[0], score[1]))\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы последовательно улучшаем модель, но с каждым разом прирост дается всё труднее. Сейчас оптимизация производится с прореживанием 30%.\n",
    "\n",
    "Можно рассмотреть верность на тестовых данных для других значений прореживания, когда в качестве оптимизатора используется Adam:\n",
    "![](https://www.safaribooksonline.com/library/view/deep-learning-with/9781787128422/assets/B06258_01_24.png)\n",
    "\n",
    "## Увеличение числа периодов (эпох)\n",
    "\n",
    "Попробуем увеличить число периодов обучения с 20 до 200. Время вычислений увеличится в 10 раз, но никакого выигрыша мы не получим. \n",
    "\n",
    "Эксперимент оказался неудачным. **Увеличение времни обучения далеко не всегда приводит к улучшению. Успех обучения обусловлен скорее применением удачных методов, а не временем, потраченным на расчеты.**\n",
    "\n",
    "![](https://www.safaribooksonline.com/library/view/deep-learning-with/9781787128422/assets/B06258_01_25.png)\n",
    "\n",
    "## Увеличение скорости обучения оптимизатора\n",
    "\n",
    "Можно также попробовать изменить скорость обучения оптимизатора. На сл. графике видно, что оптимальное значение близко к 0.001, а это как раз и есть значение задаваемое по умолчанию.\n",
    "\n",
    "Т.е. Adam работает не требуя никакой дополнительной настройки.\n",
    "\n",
    "![](https://www.safaribooksonline.com/library/view/deep-learning-with/9781787128422/assets/B06258_01_26.png)\n",
    "\n",
    "## Увеличение числа нейронов в скрытых слоях\n",
    "\n",
    "Еще одна возможность - изменение числа нейронов во внутренних скрытых слоях. На сл. графике показаны результаты, получаемые при увеличении числа нейронов, т.е. **сложности модели**.\n",
    "Время вычислений быстро растет, поскольку приходится отимизировать все больше параметров. Но достигаемый выигрыш становится все меньше и меньше.\n",
    "\n",
    "![](https://www.safaribooksonline.com/library/view/deep-learning-with/9781787128422/assets/B06258_01_27.png)\n",
    "\n",
    "На сл. графике изображено изменние времени одной итерации при росте числа скрытых нейронов.\n",
    "![](https://www.safaribooksonline.com/library/view/deep-learning-with/9781787128422/assets/B06258_01_28.png)\n",
    "\n",
    "Изменение верности при росте числа нейронов:\n",
    "![](https://www.safaribooksonline.com/library/view/deep-learning-with/9781787128422/assets/B06258_01_29.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Увеличение размера пакета (batch)\n",
    "\n",
    "Алгоритм градиентного спуска пытается минимизировать функцию стоимости одновременно на всех примерах обучающей выборки и для всех представленных в ней признаков. Алгоритм SGD обходится дешевле, поскольку в нем рассматривается тоько BATCH_SIZE примеров.\n",
    "\n",
    "Ниже приведен график поведения модели в зависимости от этого параметра. Как видим, оптимальная верность достигается, когда BATCH_SIZE = 128:\n",
    "\n",
    "![](https://www.safaribooksonline.com/library/view/deep-learning-with/9781787128422/assets/B06258_01_30.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подведение итогов экспериментов по распознаванию рукописных цифр\n",
    "\n",
    "Успешные эксперименты:\n",
    "* Добавление скрытых слоёв сети;\n",
    "* Прореживание (30%)\n",
    "* Оптимизатор: RMSProp\n",
    "* Оптимизатор: Adam\n",
    "\n",
    "Неуспешные эксперименты:\n",
    "* Рост числа внутренних нейронов влечет за собой рост модели и рост объема вычислений, но дает едва осязаемый выигрыш.\n",
    "* То же самое относится и к увеличению числа периодов обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Применение регуляризации для предотвращения переобучения\n",
    "\n",
    "Интуитивно представляется, что хорошая модель машинного обучения должна давать минимальную ошибку на обучающих данных. Математически это равносильно минимизации построенной моделью функции потерь на обучающих данных и выражается формулой:\n",
    "$$ \\min loss(Training data | Model) $$\n",
    "\n",
    "Однако, этого может оказаться недостаточно. Модель может стать избыточно сложной (**почему?**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Для выполнения сложно модели нужно много времени\n",
    "2) Сложная модель может показывать великолепное качество на обучающих данных просто потому что она запомнила все присуствующие в них связи, но гораздо худше ена контролььных - поскольку модель не обобщается на новые данные, а только запоминает данные.\n",
    "Таким образом, обобщение свелось к запоминанию\n",
    "\n",
    "На графике изображена типичная функция потерь, убывающая как на обучабем, так и контрольном наборею однако, в какой-то момент потеря на контрольных данных начинает расти из-за переобучения.\n",
    "\n",
    "![](https://www.safaribooksonline.com/library/view/deep-learning-with/9781787128422/assets/B06258_01_31.png)\n",
    "\n",
    "Эвристическое правило: если в процессе обучения мы наблюдаем возрастание потери на контролтьном наборе после первоначального убывания, значит модель слшком сложна и близком подогнана к обучающим данным.\n",
    "Это называется **переобучение**.\n",
    "\n",
    "Для решения проблемы переобучения нужно как-то выразить сложность модели и управлять ею. По существу модель - вектор весов. Поэтому её сложность можно представить в виде количества ненулевых весов. \n",
    "\n",
    "Иными словами, если модели $M_1$ и $M_2$ дают примерно одинаковое качество в терминах функции потерь, следует предпочесть ту, в которой **меньше ненулевых весов**. Для управления важностью выбора более простой модели, можно использовать параметр $\\lambda \\geq 0$ и минимизировать функцию:\n",
    "$$ \\min loss (Training Data | Model) + \\lambda complexity(Model)$$\n",
    "\n",
    "В машинном обучении применяется 3 способа регуляризации:\n",
    "* $L_1$-регуляризация\n",
    "* $L_1$-регуляризация (гребневая)\n",
    "* ElasticNet регуляризация\n",
    "\n",
    "Следует отметить, что идею регуляризации можно применить и к весам, и к модели, и к активации.\n",
    "\n",
    "Таким образом, регуляризация может способствовать повышению качества сети, особенно если налицо очевидное переобучение.\n",
    "\n",
    "*Упражнения и эксперименты самостоятельно*.\n",
    "\n",
    "Keras поддерживает все три формы регуляризации. Добавить регуляризатор просто. Ниже показано задание $L_2$-регуляризатора ядра (вектора весов $W$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import regularizers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(64, input_dim=64, kernel_regularizer=regularizers.l2(0.01)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полное описание параметров регуляризации имеется на странице: https://keras.io/regularizers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Настройка гиперпараметров\n",
    "\n",
    "Описанные эксперименты помогли составить представлеие, какие имеются способы настройки нейросети. Однако, что подходит для одного примера может не подойти в других случаях. Для каждой сети имеется много допускающих оптимизацию параметров (кол-во скрытых нейронов, размер пакета, кодличество эпох, и ряд других параметров, зависязих от сложности сети).\n",
    "\n",
    "Настройкой гиперпараметров называется процесс поиска оптимального сочетания этих параметров, при котором достигается минимум функции стоимости. Если имеется $n$ параметров, то можно считать, что они определяют $n$ мерное пространство, а наша цель - найти в этом пространстве точку, в которой функция стоимости будет приобретаеть минимальное значение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предсказание выхода\n",
    "\n",
    "Обученную сеть естественно использовать для предсказания. В Keras это доаточно просто:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для заданного входного вектора можно вычислить несколько значений:\n",
    "* model.evaluate() : вычисляет потерю\n",
    "* model.predict_classes() : вычисляет категориальные выходы;\n",
    "* model.predict_proba() : вычисляет вероятности классов.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практическое изложение алгоритма обратного распространения ошибки\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Многослойный перцептрон обучается на данных с помощью процесса, называемого **обратным распространением (backpropagation)**. Его можно описать как постоянное исправление ошибок по мере их обнаружения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С любой нейронной сетью ассоциирован набор весов, которые служат для вычисления выходных значений по входным. Кроме того, в нейронной сети может быть несколько скрытых слоев.\n",
    "\n",
    "Первоначально всем весам присваиваются случайные значения. Затем сеть активируется для каждого входного значения из обучающего набора: значения распространяются в **прямом** направлении от входного слоя через скрытые к выходному, который и выдает предсказание:\n",
    "![](https://www.safaribooksonline.com/library/view/deep-learning-with/9781787128422/assets/B06258_01_32.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку истинное наблюдаемое значение для обучающего набора известно, мы можем вычислть ошибку предсказания. Идея заключается в том, чтобы выполнить обратное распространение ошибки и с помощью подходящего алгоритма оптимизации (например, SGD) подправить веса нейросети с целью уменьшения ошибки:\n",
    "![](https://www.safaribooksonline.com/library/view/deep-learning-with/9781787128422/assets/B06258_01_33.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Процесс прямого распространения сигнала от входного слоя к выходному и обратного распространения ошибки повторяется много раз, пока ошибка не станет ниже заранее заданного порогового значения. Весь процесс изображен ниже:\n",
    "![](https://www.safaribooksonline.com/library/view/deep-learning-with/9781787128422/assets/B06258_01_34.png)\n",
    "\n",
    "Атрибуты - это входные данные, а метки служат для управления процессом обучения. Модель обновляется таким образом, что функция потерь на каждом шаге минимизируется. В нейросети важен не столько отклик отдельно взятого нейрона, сколько весь набор корректируемых весов в каждом слое. Поэтому сеть постепенно изменяет внутренние веса так, чтобы увеличить количество правильно предсказанных меток. Конечно для минимизации расзождения в процессе обучения принципиально вадно, чтобы были выбраны подходящие признаки, а данные были размечены правильно.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вперед, к глубокому обучению\n",
    "\n",
    "Мы увидели, что по мере приближения точности к 99%, тем все труднее и труднее её становится улучшить. Если мы хотим продвигаться дальше, нужны новые идеи.\n",
    "\n",
    "Важное наблюдение (применительно к задаче распознавания рукописных цифр) - мы не учитывали информацию р расположении изображения в пространстве. Приведенный выше код преобразует растровое изображени, представляющее все цифры, в плоский вектор, в котором вся протсрантсвенная информация потеряна.\n",
    "\n",
    "Однако, наш мозг работает иначе!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Архитектура Keras\n",
    "\n",
    "**Установка Keras и TensorFlow: самостоятельно**\n",
    "\n",
    "## Keras API\n",
    "Keras обладает модульной минималистской и легкорасширяемой архитектурой.\n",
    "\n",
    "С помощью Keras определяются высокоуровневые нейронные сети, работающие поверх библиотеки TensorFlow или Theano.\n",
    "\n",
    "* **Модульность**. Модель представяет собой последовательность или граф автономных модулей, легко соединяемых между собой (как детали конструктора), образуя нейросеть. В библиотеке имеется множество готовых модулей, реализующих различные типы слоев, функций стоимости, оптимизаторов, схем инициализации, функций активации и методов регуляризации.\n",
    "* **Минимализм**. Библиотека разработана на Python, все модули короткие и самодокументированные.\n",
    "* **Расширяемость**. В библиотеку можно добавлять новую функциональность.\n",
    "\n",
    "## Введение в архитектуру Keras\n",
    "\n",
    "### Понятие тензора\n",
    "Keras использует библиотеку Theano, или TensorFlow для эффективных вычислений с тензорами. **Тензор - это многомерный массив, или обобщенная матрица.** Обе библиотеки умеют эффективно выполнять символические вычисления с тензорами, которые являются основным строительным блоком для создания нейронных сетей.\n",
    "\n",
    "### Соединение моделей Keras\n",
    "В Keras есть два способа соединения моделей:\n",
    "* Последовательная композиция\n",
    "* Фукциональная композщиция\n",
    "\n",
    "#### Последовательная композиция\n",
    "В этом случае готовые модели соединяются в линейный конвейер слоев, напоминающий стек, или очередь. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 3)                 387       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 117,379\n",
      "Trainable params: 117,379\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(784,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функциональная композиция\n",
    "Функциональный API позволяет определять более сложные модели, например, ациклические графы, модели с разделяемыми слоями, или с несколькими выходами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обзор готовых слоев нейросетей\n",
    "\n",
    "Keras предоставляет несколько готовых слоев Рассмотрим наиболее употребительные.\n",
    "\n",
    "### Обычный плотный слой (Dense layer)\n",
    "Плотная модель - это полносвязый слой нейронной сети. Ниже приведен прототип модели со всеми параметрами.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras.layers.core.Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Рекуррентные нейронные сети - простая, LSTM и GRU\n",
    "рекуррентные нейронные сети - класс сетей, в которых используется последовательная природа входных данных. Вхзодными данными может быть текст, речь, временные ряды и вообще лбюбой обхект, в котором появление элемента в последовательности зависит от предшествующих элементов. \n",
    "\n",
    "Позже мы будем обсуждать рекуррентные сети трех видов: простые, LSTM и GRU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras.layers.recurrent.Recurrent(return_sequences=False, go_backwards=False, stateful=False, unroll=False, implementation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keras.layers.recurrent.SimpleRNN(units, activation='tanh', use_bias=True, \n",
    "                                 kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', \n",
    "                                 bias_initializer='zeros', kernel_regularizer=None, \n",
    "                                 recurrent_regularizer=None, bias_regularizer=None, \n",
    "                                 activity_regularizer=None, kernel_constraint=None, \n",
    "                                 recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keras.layers.recurrent.GRU(units, activation='tanh', recurrent_activation='hard_sigmoid', \n",
    "                           use_bias=True, kernel_initializer='glorot_uniform', \n",
    "                           recurrent_initializer='orthogonal', bias_initializer='zeros', \n",
    "                           kernel_regularizer=None, recurrent_regularizer=None, \n",
    "                           bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, \n",
    "                           recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keras.layers.recurrent.LSTM(units, activation='tanh', recurrent_activation='hard_sigmoid', \n",
    "                            use_bias=True, kernel_initializer='glorot_uniform', \n",
    "                            recurrent_initializer='orthogonal', bias_initializer='zeros', \n",
    "                            unit_forget_bias=True, kernel_regularizer=None, \n",
    "                            recurrent_regularizer=None, bias_regularizer=None, \n",
    "                            activity_regularizer=None, kernel_constraint=None, \n",
    "                            recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сверточные и пулинговые слои\n",
    "Сверточные сети - класс нейронных сетей, в которых сверточные и пуллинговые операции используются дл постепенного обучения довольно сложных моделей с повышающимся уровем абстракции. Такой способ обучения напоминает модель человеческого зрения, сложившуюся в результате миллионов лет эволюции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keras.layers.convolutional.Conv1D(filters, kernel_size, strides=1, padding='valid', \n",
    "                                  dilation_rate=1, activation=None, use_bias=True, \n",
    "                                  kernel_initializer='glorot_uniform', bias_initializer='zeros', \n",
    "                                  kernel_regularizer=None, bias_regularizer=None, \n",
    "                                  activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
    "\n",
    "keras.layers.convolutional.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', \n",
    "                                  data_format=None, dilation_rate=(1, 1), activation=None, \n",
    "                                  use_bias=True, kernel_initializer='glorot_uniform', \n",
    "                                  bias_initializer='zeros', kernel_regularizer=None, \n",
    "                                  bias_regularizer=None, activity_regularizer=None,\n",
    "                                  kernel_constraint=None, bias_constraint=None)\n",
    "\n",
    "keras.layers.pooling.MaxPooling1D(pool_size=2, strides=None, padding='valid')\n",
    "\n",
    "keras.layers.pooling.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Регуляризация\n",
    "Цель регуляризации - предотвращение переобучения. В слоях разных типов имеются параметры регуляризации. Ниже приведен список параметров регуляризации, часто используемых в сверточных и плотных модулях.\n",
    "\n",
    "* kernel_regularizer: функция регуляризации, применяемая к матрице весов;\n",
    "* bias_regularizer: функция регуляризации применяемая к вектору смещений\n",
    "* activity_regularizer: функция регуляризации, применяемая к выходу слоя (его функции активации.\n",
    "\n",
    "Кроме того, для регуляризации можно использовать прореивание и зачастую это дает весомый эффект:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keras.layers.core.Dropout(rate, noise_shape=None, seed=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "где:\n",
    "* rate - вещественное число в интервале [0;1]. показывающее какую часть входных блоков отбросить;\n",
    "* noise_shape - одномерный целочисленный тензор, задающий форму двоичной маски прореживания, которая умножается на входной сигнал;\n",
    "* seed - целое число, служащее для инифицализации генератора случайных чисел.\n",
    "\n",
    "### Пакетная нормировка\n",
    "\n",
    "**Пакетная нормировка** позволяет ускорить обучение и в общем случае получить большую верность.\n",
    "Ниже прототип с параметрами:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras.layers.normalization.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обзор предопределенных функций активации\n",
    "К числу готовых функций активации относятся, в частности:\n",
    "* сигмоида;\n",
    "* линейная функция;\n",
    "* гиберболический танкенс;\n",
    "* блок линейной ректификации (ReLU);\n",
    "\n",
    "![](https://www.safaribooksonline.com/library/view/deep-learning-with/9781787128422/assets/image_02_023.png)\n",
    "\n",
    "### Обзор функций потерь\n",
    "\n",
    "Функции потерь (целевые функции), используемые в Keras (link: https://keras.io/losses/ )\n",
    "можно отнести к четырем категориям:\n",
    "* Верность, используемая в задачах классификации. Таких функций четыре:\n",
    "    * binary_accuracy (средняя верность по всем предсказаниям в задачах бинарной классификации);\n",
    "    * categorial_accuracy (средняя верность по всем предсказаниям в задачах многоклассовой классификации);\n",
    "    * sparse_categorial_accuracy (используется, когда метки разреженные);\n",
    "    * top_k_categorial_accuracy (успехом считается случай, когда истинный целевой класс находится среди первых top_k предсказаний);\n",
    "* Ошибка, измеряющая различие между предсказанными и фактическими значениями. варианты таковы: mse (среднеквадратичная ошибка), rmse (квадратный корень среднеквадратичной ошибки), mae (средняя абсолютная ошибка), mape (средняя ошибка в процентах), msle (средняя квадратично-логарифмическая ошибка).\n",
    "* Кусочно-линейная функция потерь, которая обычно применяется для обучения классификаторов. Существуют два варианта: *кусочно-линейная*, определяемая как $\\max(1-y_{true}*y_{pred},0)$ и *квадратичная кусочно-линейная*, равная квадрату кусочно-линейной.\n",
    "* Классовая потеря используется для вычисления перекрестной энтропии в задачах классификации. Существует несколько вариантов, включая бинарную перекрестную энтропию и категориальную перекрестную энтропию.\n",
    "\n",
    "### Обзор показателей качества\n",
    "https://keras.io/metrics/\n",
    "\n",
    "### Обзор оптимизаторов\n",
    "https://keras.io/optimizers/\n",
    "\n",
    "## Некоторые полезные операции\n",
    "\n",
    "### Сохранение и загрузка весов и архитектуры модели.\n",
    "Для сохранения и загрузки архитектуры модели служат сл. функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save as JSON \n",
    "json_string = model.to_json()\n",
    "# save as YAML \n",
    "yaml_string = model.to_yaml() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model reconstruction from JSON: \n",
    "from keras.models import model_from_json \n",
    "model = model_from_json(json_string) \n",
    "# model reconstruction from YAML \n",
    "model = model_from_yaml(yaml_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для сохранения и загрузки параметров модели служат сл. функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model \n",
    "# creates a HDF5 file 'my_model.h5'\n",
    "model.save('my_model.h5')\n",
    "del model # deletes the existing model\n",
    "\n",
    "# returns a compiled model\n",
    "# identical to the previous one \n",
    "model = load_model('my_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
